{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab02.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EVjo9ZqdPM-O"},"source":["# Wstęp\n","## Cel ćwiczenia\n","Na tych zajęciach napiszesz od zera prosty klasyfikator liniowy - **Perceptron**. Rzadko kiedy programując **sieci neuronowe** musimy się posuwać do implementowania czegoś od zera. Jednak tym razem to zrobimy, żeby lepiej zrozumieć i opanować idee **propagacji w przód** (ang. *forward propagation*) i **propagacji wstecznej** (ang. *backward propagation*), a także zmierzyć się z problemami na jakie natrafiamy programując sieci neronowe (np. problemy ze **stabilnością numeryczną**), czy też dowiedzieć się w jaki sposób bronić się przed **przeuczeniem** (ang. *overfitting*) modelu.\n","## Warunki zaliczenia\n","W celu zaliczenia ćwiczeń należy uzupełnić wszystkie brakujące elementu kodu, wykonać wszystkie polecenia i wyuczyć model.\n"]},{"cell_type":"code","metadata":{"id":"Mx6bXDUnANIg"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors\n","import sklearn.datasets\n","import torch\n","\n","from sklearn.preprocessing import normalize, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","assert torch.cuda.is_available(), \"Uruchom środowisko wykonawcze na GPU\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8RucDzN4Ynv"},"source":["# Problem \n","\n","Na tych zajęciach będziemy pracować na jednym z najbardziej znanych i oklepanych zbiorów danych, które są wykorzystywane do prezentacji technik z uczenia maszynowego - Irysy: [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set), [scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset).\n","\n","Jest to zbiór danych zawierający informację o długości i szerokości płatków i działek kielicha dla 3 gatunków irysów:\n","\n","*   Iris Setosa (0)\n","*   Iris Versicolour (1)\n","*   Iris Virginica (2)\n","\n","Naszym zadaniem jest stworzenie klasyfikatora, który dla podanych cech $(x_0, x_1, x_2, x_3)$ przypisze klasę $\\{0, 1, 2\\}$, tj. fukncji $f: \\mathbb{R}^4 \\to \\{0, 1, 2\\}$.\n","\n","Załadujmy zbiór danych."]},{"cell_type":"code","metadata":{"id":"8PEeJyMHVHbk"},"source":["X, y = sklearn.datasets.load_iris(return_X_y=True)\n","X[:2], y[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Us33p6lU_N-z"},"source":["# Liniowa separowalność problemu\n","\n","Na początek omówmy koncepcję istotną z punktu widzenia głębokich sieci neuronowych - separowalność liniową. Skupmy się na razie na dwóch cechach irysów: długości i szerokości działek kielichów. Zobaczmy jak te chechy rozkładają się na wykresie."]},{"cell_type":"code","metadata":{"id":"h-S4BMr64Ra3"},"source":["scat = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap([\"#E69F00\", \"#56B4E9\", \"#D55E00\"]))\n","plt.colorbar(scat, spacing='proportional',ticks=np.linspace(0,3,4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXI9wGneEcfr"},"source":["Patrząc na powyższy wykres ***wyznacz*** granicę decyzyjną pomiędzy *Iris Setosa* (0) a dwoma pozostałymi irysami. Wyznaczona granica decyzyjna powinna być taka, że powyżej jej są tylko *Iris Setosa* (0) a poniżej - pozostałe dwa gatunki."]},{"cell_type":"code","metadata":{"id":"d9VddXhlrJ8e"},"source":["scat = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap([\"#E69F00\", \"#56B4E9\", \"#D55E00\"]))\n","plt.colorbar(scat, spacing='proportional',ticks=np.linspace(0,3,4))\n","\n","x = np.arange(4, 8)\n","plt.plot(x, ___*x - ___)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSa1lhkuEwtM"},"source":["Zauważ, że poprawnych granic decyzyjnych jest nieskończenie wiele. Zobacz także, że nie bylibyśmy wstanie rodzielić między sobą dwa pozostałe gatunki: *Iris Versicolour* (1) oraz *Iris Virginica* (2).\n","\n","Jak widać przedstawiony problem (*Iris Setosa* czy nie-*Iris Setosa*) jest liniowo separowalny - istnieje linia, która rozdziela te dwie klasy ze względu na długość i szerokość działki kielicha. Uogólniając, problem jest liniowo seperowalny wtedy i tylko wtedy, gdy w $n$ wymiarowej przestrzeni jesteśmy wstanie wyznaczyć $n-1$ wymiarową płaszczyznę rozdzielającą klasy. W tym przypadku mieliśmy $2$-wymiarową przestrzeń, w której trzeba było wyznaczyć $1$-wymiarową linię."]},{"cell_type":"markdown","metadata":{"id":"a2kFYlpHVuJA"},"source":["# Przeuczenie - podział na zbiór uczący, walidacyjny i testowy\n","Jednym z podstawowych probemów z jakimi trzeba się zmierzyć przy uczeniu klasyfikatorów jest przeuczenie, tj. sytuacja w której klasyfikator bardzo dobrze nauczy się rozpoznawać próbki, które dostał w procesie uczenia, ale już gorzej będzie rozponawał nowe dane.\n","\n","Żeby uchronić się przed taką sytuacją często dokonujemy podziału zbioru danych na zbiory: **uczący, walidacyjny i testowy** (ang. *train, validation, test*). Zbiór uczący służy do wyznaczenia **parametrów** modelu, walidaycyjny - do strojenia **hiperparametrów** modelu (np. liczba iteracji uczenia), a testowy - do ostatecznego przetestowania jakości naszego modelu. Czasami ograniczamy się tylko do zbioru uczącego i testowego, jak planujemy tylko uczyć parametry modelu.\n","\n","***Podziel*** zbiór z irysami na dwa zbiory - uczący i testowy. Umieść 20% próbek w zbiorze testowym a 80% w uczącym. Dokonaj ***stratyfikacji*** danych. Użyj fukncji `sklearn.model_selection.train_test_split`. Ustaw parametr `random_state` na $1$, żeby mieć jeden ustalony podział. Zapoznaj się z [dokumentacją](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20test#sklearn.model_selection.train_test_split) przed przystąpieniem do zadania."]},{"cell_type":"code","metadata":{"id":"8RT5poS0i23w"},"source":["from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","X_train, X_test, y_train, y_test = ___ # Użyj funkcji sklearn.model_selection.train_test_split\n","\n","X_train = torch.tensor(X_train, dtype=torch.float).cuda()\n","X_test = torch.tensor(X_test, dtype=torch.float).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1u0ap0EBkwz"},"source":["# Jeden-vs-reszta\n","\n","Jak zaraz zobaczymy, perceptron z sigmoidem zwraca wartości z przedziału $[0,1]$, które to nazywamy **pseudoprawdopodobieństwami**. Na przykład, jeżeli klasyfikator zwraca wartość $0.7$ dla danej próbki, to oznacza to, że na 70% jest to klasa 1, a na 30% klasa 0.\n","\n","A więc trzymając się tej konwencji, jak tutaj \"zmieścić\" 3 klasy (0, 1, 2)? Jednym z podejść jest jeden-vs-reszta (ang. *one-vs-rest*, *OVR*). Przyjmujemy, że jedna z klas zostaje jedynką a resztę oznaczamy jako 0. Na przykład klasę 2 przemianowujemy na 1, a 0 i 1 na 0. Zresztą to co zrobiliśmy w rozdziale *Liniowa separowalność problemu* to jest dokładnie to - skupiliśmy się na klasyfikacji czy dana roślina to jeden określony gatunek irysów czy może których z dwóch pozostałych. I tak też będziemy robić w dalszej części laboratorium.\n","\n","***Uzupełnij*** i uruchom poniższy kod."]},{"cell_type":"code","metadata":{"id":"4WMjiTRiEsYo"},"source":["def get_ovr_labels(y_train, y_test):\n","    y_train_ovr = {}\n","    y_test_ovr = {}\n","    for clas in range(3):\n","        y_train_ovr[clas] = torch.tensor([1 if x == ___ else 0 for x in y_train]).cuda()\n","        y_test_ovr[clas] = torch.tensor([1 if x == ___ else 0 for x in y_test]).cuda()\n","    \n","    return y_train_ovr, y_test_ovr\n","\n","y_train_ovr, y_test_ovr = get_ovr_labels(y_train, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiYCuD0fW4ie"},"source":["Sprawdź czy dobrze zaimplementowałaś(eś) funkcję."]},{"cell_type":"code","metadata":{"id":"dD4VCvXeW8lk"},"source":["print(y_test)\n","for clas in range(3):\n","    print(y_test_ovr[clas])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8HTg6QNXkM0"},"source":["# Perceptron\n","\n","Twoim zadaniem będzie napisanie Perceptrona - podstawowej \"komórki\" sieci neuronowej."]},{"cell_type":"markdown","metadata":{"id":"GAX-tS_uZplw"},"source":["## Sigmoid\n","\n","Spójrz na poniższy kod implementujący funkcję sigmoid ze wzoru $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ dla $x \\geq 0$ oraz $\\sigma(x) = \\frac{e^x}{e^x + 1}$ dla $x < 0$."]},{"cell_type":"code","metadata":{"id":"yEGrHy7BaOkw"},"source":["def sigmoid_scalar(x):\n","    if x >= 0:\n","      return 1 / (1 + torch.exp(-x))\n","    return torch.exp(x) / (torch.exp(x) + 1)\n","\n","def sigmoid_tensor(t):\n","    return torch.tensor([sigmoid_scalar(x) for x in t])\n","\n","sigmoid_test_tensor = torch.tensor([-100.0, 100.0, -1000.0, 1000.0, 0.0, 1.0, -1.0, 100000000.0, -100000000.0]).cuda()\n","sigmoid_tensor(sigmoid_test_tensor).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-JA8FbQcMup"},"source":["Jeżeli nie widzisz dla czego trzeba rodzielić liczenie sigmoidu na dwa przypadki to spróbuj policzyć $\\sigma(-100)$ i $\\sigma(100)$ korzystając z obu wzorów. Czym się różnią wyniki? Dlaczego?"]},{"cell_type":"code","metadata":{"id":"vCf5fWnUcMEg"},"source":["print(1 / (1 + torch.exp(-sigmoid_test_tensor[0])))\n","print(torch.exp(sigmoid_test_tensor[1]) / (torch.exp(sigmoid_test_tensor[1]) + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyJuIdQkdbZ8"},"source":["Niestety, ale problemy ze **stablinością numeryczną** w sieciach neuronowych są dość powszechne. Na dziś wystarczy tylko, że zapamiętasz, że pośrednie wyniki obliczeń mogą być za małe albo za duże, żeby zapisać je do zmiennoprzecinkowej postaci (ang. *floating-point format*).\n","\n","Ta funkcja ma jeszcze jeden problem. Obecnie **iterujmey w pętli** po elementach tensora zamiast wykonywać operacje matematyczne (dodawanie, mnożenie etc.) na tensorach, co jest znacznie wolniejsze i mniej eleganckie.\n","\n","Jeżeli chcemy **efektywnie** używać sigmoida do uczenia musimy zmienić naszą implementację. Dlatego też skorzystamy z innego wzoru $\\sigma(x) = \\exp(-\\ln(\\exp(0) + \\exp(-x)))$. Ten wzór ma jedną podstawową zaletę - możemy skorzystać z numercznie stabilnego [`torch.logaddexp`](https://pytorch.org/docs/stable/generated/torch.logaddexp.html).\n","\n","***Zaimplementuj*** funkcję sigmoid używając wzoru $\\sigma(x) = \\exp(-\\ln(\\exp(0) + \\exp(-x)))$ oraz funkcji [`torch.logaddexp`](https://pytorch.org/docs/stable/generated/torch.logaddexp.html)."]},{"cell_type":"code","metadata":{"id":"yv9MF4FAjfD9"},"source":["def sigmoid(x):\n","    return ___\n","\n","s = sigmoid(sigmoid_test_tensor).cuda()\n","\n","print(s)\n","assert isinstance(s, torch.cuda.FloatTensor)\n","assert s.shape == (9,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMQSTcA8EK1q"},"source":["Dla pewności jeszcze sprawdzimy czy na pewno implementacja z użyciem operacji na tensorach jest szybsza."]},{"cell_type":"code","metadata":{"id":"mOmtyAceBpFw"},"source":["print(\"*** Rozwiązanie iteracyjne ***\")\n","%timeit sigmoid_tensor(torch.rand(10000).cuda())\n","print()\n","print(\"*** Rozwiązanie z użyciem operacji na tensorach ***\")\n","%timeit sigmoid(torch.rand(10000).cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MIKL_yPkUoV"},"source":["Nawet przy implementacji tak prostej funkcji mieliśmy dwa wyzwania, którym trzeba było sprostać: (1) zapewnić **stabilność numerczyną** operacji oraz (2) zapewnić, że będziemy wykonywać **efektywne operacje na wektorach**. Są to dwie istotne kwestie, o których należy pamiętać projektując sieci neuronowe."]},{"cell_type":"markdown","metadata":{"id":"zPX8wLVfaUoR"},"source":["## Inicjalizacja parametów\n","\n","***Napisz*** funkcję, która zwróci krotkę dwuelementową - pierwszym elementem będzie tensor zawierający **parametry** modelu - na razie same zera - oraz parametr b (ang. *bias*) - też równy zero."]},{"cell_type":"code","metadata":{"id":"0tp1jETgadRn"},"source":["def initialize_with_zeros(length):\n","    w = ___\n","    b = ___\n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUfxJbrznK9x"},"source":["***Przetestuj*** swoją implementację. Kod poniżej powinien wykonać się bez błędów i wyjątków"]},{"cell_type":"code","metadata":{"id":"O0rcFgR9mlGA"},"source":["w, b = initialize_with_zeros(42)\n","\n","assert isinstance(w, torch.cuda.FloatTensor)\n","assert isinstance(b, torch.cuda.FloatTensor)\n","assert w.shape == (42,)\n","assert not [x for x in w if x != 0]\n","assert b.shape == (1,)\n","assert b == 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwGAsVqkmZM4"},"source":["## Propagacja wsteczna i propagacja w przód\n","\n","Niech $m$ będzie liczbą próbek, na których uczymy model.\n","\n","***Zaimplementuj*** funkcję `activation_fn`. Policz funkcję aktywacji ze wzroru $A = \\sigma(Xw + b) = (a_0, a_1, \\ldots, a_{m-1})$.\n","\n","***Nie korzystaj*** z pętli, tylko z operacji na wektorach/macierzach."]},{"cell_type":"code","metadata":{"id":"x7oS4u9Khq9i"},"source":["def activation_fn(w, b, X):\n","    return ___"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pTCQVsynhry7"},"source":["***Przetestuj kod***. W tym celu ***zamień*** `___` na taką liczbę, żeby kod zadziałał poprawnie. Zastanów się dlaczego akurat taka liczba parametrów jest potrzebna a nie inna?"]},{"cell_type":"code","metadata":{"id":"N44DcxF5h5ce"},"source":["w, b = initialize_with_zeros(___)\n","y_pred_prob = activation_fn(w, b, X_train[:10])\n","print(y_pred_prob)\n","\n","assert isinstance(y_pred_prob, torch.cuda.FloatTensor)\n","assert y_pred_prob.shape == (10,)\n","assert not [x for x in y_pred_prob if x != 0.5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bt8EpbWliQLH"},"source":["Powinieneś/powinnaś dostać:\n","\n","```\n","tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n","        0.5000], device='cuda:0')\n","```\n","\n","Zauważ, że to oznacza, że dla każdej z 10 próbek model przyporządkował pseudoprawdopodobieństwo równe $0.5$. Jest to zrozumiałe, ponieważ model na razie nie posiada żadnej wiedzy, dlatego też przypisuje takie samo prawdopodbieństwo klasie 1 i 0 dla każdej próbki.\n","\n","***Zaimplementuj*** funkcję `forward` dokonującą propagacji w przód. Wylicz wartość funkcji aktywacji korzystając z wcześniej zaimlepementowanej funkcji `activation_fn`. Funkcję kosztu policz ze wzoru $loss = - \\frac{1}{m} \\sum^{m-1}_{i=0} y_i\\log(a_i) + (1 - y_i)log(1 - a_i)$.\n","\n","***Zaimplementuj*** funkcję `backward` dokonującą propagacji wstecznej. Policz gradient dla wektora $w$ ze wzoru $ \\frac{\\partial loss}{\\partial w} = \\frac{1}{m}EX$ (gdzie $E$ to błąd, $E=A-Y$), a gradient dla $b$ ze wzoru $ \\frac{\\partial loss}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a_i-y_i)$.\n","\n","***Nie korzystaj*** z pętli, tylko z operacji na wektorach/macierzach."]},{"cell_type":"code","metadata":{"id":"DO2XpaxAn6FI"},"source":["def propagate(w, b, X, y):\n","    m = torch.tensor([X.shape[0]]).cuda()\n","\n","    def forward():\n","        A = activation_fn(w, b, X)\n","        loss = ___ # Policz koszt\n","        return A, loss\n","\n","    def backward(A):\n","        error = ___ # Policz błąd modelu\n","        dw = ___ # Policz gradient dla w (parametrów)\n","        db = ___ # Policz gradient dla b (bias)\n","        return dw, db\n","\n","    A, loss = forward()\n","    dw, db = backward(A)\n","\n","    return dw, db, loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qC4EnD2yl9ao"},"source":["***Przetestuj kod***. W tym celu ***zamień*** `___` na taką liczbę, żeby kod zadziałał poprawnie."]},{"cell_type":"code","metadata":{"id":"-qKzU648l8ZU"},"source":["w, b = initialize_with_zeros(___)\n","dw, db, loss = propagate(w, b, X_train[:10], y_train_ovr[1][:10])\n","print(dw, db, loss)\n","\n","assert isinstance(dw, torch.cuda.FloatTensor)\n","assert isinstance(db, torch.cuda.FloatTensor)\n","assert isinstance(loss, torch.cuda.FloatTensor)\n","assert torch.allclose(dw, torch.tensor([-0.0700,  0.0650, -0.1600, -0.0050]).cuda(), atol=1e-04, rtol=0)\n","assert torch.allclose(b, torch.tensor([0.0]).cuda(), atol=1e-04, rtol=0)\n","assert torch.allclose(loss, torch.tensor([0.6931]).cuda(), atol=1e-04, rtol=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kus3zWoAmauj"},"source":["\n","Powinnaś(eś) dostać:\n","\n","```\n","tensor([-0.0700,  0.0650, -0.1600, -0.0050], device='cuda:0',\n","       dtype=torch.float64) tensor([0.], device='cuda:0', dtype=torch.float64) tensor([0.6931], device='cuda:0', dtype=torch.float64)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XR-3_dUYnEja"},"source":["## Uczenie\n","\n","***Zaprogramuj*** iteracyjny proces uczenia modelu, tzn. powtarzaj proces propagacji w przód i propagacji wstecznej `num_iterations` razy. Podczas aktulizacji parametrów $w$ i $b$ ***uwzględnij*** `learning_rate` tj. współczynnik uczenia, który mówi nam o tym jak \"szybko\" chcemy podąrzać za zwrotem gradientu. Zauważ, że `num_iterations` i `learning_rate` to hiperparametry modelu, a $w$ i $b$ to jego parametry."]},{"cell_type":"code","metadata":{"id":"R8fkF93PokQg"},"source":["def train(w, b, X, Y, num_iterations, learning_rate, print_loss=False):\n","    losses = []\n","\n","    for i in range(num_iterations):\n","        dw, db, loss = ___\n","        w = ___\n","        b = ___\n","        \n","        losses.append(loss)\n","        if print_loss:\n","            print(f\"Loss after iteration {i}: {loss}\")\n","\n","    return w, b, losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qq2Td8M1u417"},"source":["***Przetestuj*** działanie napisanego przez siebie kodu"]},{"cell_type":"code","metadata":{"id":"5sh3HNNIuaH-"},"source":["w, b = initialize_with_zeros(___)\n","w, b, losses = train(w, b, X_train[:10], y_train_ovr[1][:10], 5, 0.005)\n","print(w, b, losses)\n","\n","assert isinstance(w, torch.cuda.FloatTensor)\n","assert isinstance(b, torch.cuda.FloatTensor)\n","assert torch.allclose(w, torch.tensor([1.4422e-03, -1.7717e-03, 3.7738e-03, 4.6234e-05]).cuda(), atol=1e-04, rtol=0)\n","assert torch.allclose(b, torch.tensor([-5.0243e-05]).cuda(), atol=1e-04, rtol=0)\n","assert torch.allclose(torch.tensor(losses), torch.tensor([0.6931, 0.6930, 0.6928, 0.6927, 0.6925]), atol=1e-04, rtol=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgvS2u21u7ak"},"source":["Powinno być:\n","```\n","tensor([ 1.4422e-03, -1.7717e-03,  3.7738e-03,  4.6234e-05], device='cuda:0',\n","       dtype=torch.float64) tensor([-5.0243e-05], device='cuda:0', dtype=torch.float64) [tensor([0.6931], device='cuda:0', dtype=torch.float64), tensor([0.6930], device='cuda:0', dtype=torch.float64), tensor([0.6928], device='cuda:0', dtype=torch.float64), tensor([0.6927], device='cuda:0', dtype=torch.float64), tensor([0.6925], device='cuda:0', dtype=torch.float64)]\n","```"]},{"cell_type":"markdown","metadata":{"id":"ocD1dQo5xfRh"},"source":["## Pierwsza klasyfikacja na danych testowych\n","\n","***Napisz*** funkcję dokonującą klasyfikacji z użyciem parametrów $w$ i $b$ oraz przekazanych cech próbek $X$. Wykorzystaj wcześniej napisaną funkcję aktywacji. `y_pred` da się policzyć w jednej linii bez iterowania po tensorze. "]},{"cell_type":"code","metadata":{"id":"nL4kzxRxv-zH"},"source":["def predict(w, b, X):\n","    A = ___\n","    y_pred = ___\n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmJ7KxT-v0zy"},"source":["Zauważ, że użyliśmy ponownie funkcji `activation_fn`. Dobrą praktyką w **inżynierii oprogramowania** jest wydzielanie powtarzających się fragmentów kodu (np. do osobnej funkcji) i ich ponowne używanie.\n","\n","***Przetestuj*** czy Twoja implementacja działa poprawnie, tzn. zwraca tablicę z klasyfiakcją dla poszczególnych próbek (na razie nie musi być trafna)."]},{"cell_type":"code","metadata":{"id":"w2m_FULawoF7"},"source":["w, b = initialize_with_zeros(___)\n","w, b, losses = train(w, b, X_train[:10], y_train_ovr[2][:10], 5, 0.005)\n","y_test_2 = y_test_ovr[2][:10]\n","y_test_2_pred = predict(w, b, X_test[:10])\n","\n","print(y_test_2)\n","print(y_test_2_pred) \n","\n","assert torch.equal(y_test_2_pred, torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NXurNtbxjQG"},"source":["## Kompletny model\n","\n","Napisz funkcję `validate`, która:\n","\n","\n","1.   Zainicjalizuje parametry $w$ i $b$.\n","2.   Nauczy się parametrów na podstawie `X_train` i `y_train` w `num_iterations` iteracjach z współczynnikiem uczenia (\"prędkością\") określoną przez `learning_rate`.\n","3.   Dokona klasyfikacji próbek z `X_train` i `X_test`.\n","4.   Sprawdzi dokładność (ang. *accuracy*) klasyfikacji na obu podzbiorach.\n","\n"]},{"cell_type":"code","metadata":{"id":"EQxb9korDqjq"},"source":["def validate(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.5, print_loss=False):\n","    ___\n","\n","    y_pred_train = ___\n","    y_pred_test = ___\n","\n","\n","    print(\"Train accuracy: {} %\".format(accuracy_score(y_train.cpu(), y_pred_train.cpu())))\n","    print(\"Test accuracy: {} %\".format(accuracy_score(y_test.cpu(), y_pred_test.cpu())))\n","\n","    d = {\"losses\": losses,\n","        \"weights\": w,\n","        \"bias\": b,\n","        \"y_pred_test\": y_pred_test,\n","        \"y_pred_train\": y_pred_train,\n","        \"learning_rate\": learning_rate,\n","        \"num_iterations\": num_iterations}\n","\n","    return d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRZDoAZGyB72"},"source":["Sprawdź jakość swojego klasyfikatora:"]},{"cell_type":"code","metadata":{"id":"jvcmbi6b1e0j"},"source":["def test_all(X_train, y_train_ovr, X_test, y_test_ovr, num_iterations=10000, learning_rate=0.005, print_loss=False):\n","    for clas in range(3):\n","        print(f\"Class {clas}:\")\n","        validate(X_train, y_train_ovr[clas], X_test, y_test_ovr[clas], num_iterations=num_iterations, learning_rate=learning_rate, print_loss=print_loss)\n","\n","print(\"-- 100 iterations --\")\n","test_all(X_train, y_train_ovr, X_test, y_test_ovr, num_iterations=100, learning_rate=0.005, print_loss=False)\n","print(\"-- 600 iterations --\")\n","test_all(X_train, y_train_ovr, X_test, y_test_ovr, num_iterations=600, learning_rate=0.005, print_loss=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bdHfduZy8kX"},"source":["# Przetwarzanie wstępne\n","\n","Teraz zajmiemy się czymś czym powinniśmy się zająć na samym początku - **przetwarzaniem wstępnym** (ang. *preprocessing*). Sieci neuronowe zazwyczaj nie radzą sobie dobrze z danymi wrzuconymi \"od tak\".\n","\n","***Dokonaj*** standaryzacji cech, tzn. takiego ich przekształcenia, żeby ich średnia wynosiła zero a wariancja - jeden. Wykorzystaj do tego [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale) (już zaimporotowany)."]},{"cell_type":"code","metadata":{"id":"evSC-yEZ0tKA"},"source":["scaler = ___ # Stwórz instancję klasy StandardScaler oraz uruchom funkcję fit na zbiorze uczącym\n","X_train_std = torch.tensor(scaler.transform(X_train.cpu()), dtype=torch.float).cuda()\n","X_test_std = torch.tensor(scaler.transform(X_test.cpu()), dtype=torch.float).cuda()\n","\n","print(\"-- 10 iterations without scaler --\")\n","test_all(X_train, y_train_ovr, X_test, y_test_ovr, num_iterations=10, learning_rate=0.005, print_loss=False)\n","print(\"-- 10 iterations with scaler --\")\n","test_all(X_train_std, y_train_ovr, X_test_std, y_test_ovr, num_iterations=10, learning_rate=0.005, print_loss=False)\n","print()\n","print(\"-- 10000 iterations without scaler --\")\n","test_all(X_train, y_train_ovr, X_test, y_test_ovr, num_iterations=10000, learning_rate=0.005, print_loss=False)\n","print(\"-- 10000 iterations with scaler --\")\n","test_all(X_train_std, y_train_ovr, X_test_std, y_test_ovr, num_iterations=10000, learning_rate=0.005, print_loss=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3eCDlAIH3xoB"},"source":["Zauważ, że:\n","1.   Uczenia `StandardScaler`'a dokonaliśmy na zbiorze uczącym. Nie możemy wykorzystywać informacji ze zbioru testowego, ponieważ moglibyśmy przeuczyć model.\n","2.   Po dokonaniu standaryzacji model znacznie szybciej się uczy, ale dla większej liczby iteracji wpływ przetwarzania wstępnego nie jest oczywisty.\n","\n","Tak więc trzeba pamiętać, że (1) przy przetwarzaniu wstępnym także należy uważać na potencjalne przeuczenie oraz (2) to jakie przetwarzanie wstępne należy dokonać zależy w dużej mierze od tego jakie mamy **dane**, jaki **problem** mamy do rozwiązania i przy jakich **zasobach**."]}]}