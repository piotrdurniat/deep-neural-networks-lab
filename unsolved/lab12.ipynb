{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRya3-1t7gO7"
      },
      "source": [
        "# Wstęp\n",
        "\n",
        "Metody uczenia maszynowego możemy podzielić na dwie główne kategorie (pomijając uczenie ze wzmocnieniem): nadzorowane i nienadzorowane. Uczenie **nadzorowane** (ang. *supervised*) to jest uczenie z dostępnymi etykietami dla danych wejściowych. Na parach danych uczących $dataset= \\{(x_0,y_0), (x_1,y_1), \\ldots, (x_n,y_n)\\}$ model ma za zadanie nauczyć się funkcji $f: X \\rightarrow Y$. Z kolei modele uczone w sposób **nienadzorowany** (ang. *unsupervised*) wykorzystują podczas trenowania dane nieetykietowane tzn. nie znamy $y$ z pary $(x, y)$.\n",
        "\n",
        "Dość częstą sytuacją, z jaką mamy do czynienia, jest posiadanie małego podziobioru danych etykietowanych i dużego nieetykietowanych. Często annotacja danych wymaga ingerencji człowieka - ktoś musi określić co jest na obrazku, ktoś musi powiedzieć czy dane słowo jest rzeczownkiem czy czasownikiem itd.\n",
        "\n",
        "Jeżeli mamy dane etykietowane do zadania uczenia nadzorowanego (np. klasyfikacja obrazka), ale także dużą ilość danych nieetykietowanych, to możemy wtedy zastosować techniki **uczenia częściowo nadzorowanego** (ang. *semi-supervised learning*). Te techniki najczęściej uczą się funkcji $f: X \\rightarrow Y$, ale jednocześnie są w stanie wykorzystać informacje z danych nieetykietowanych do poprawienia działania modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjjlvGdZNg00"
      },
      "source": [
        "## Cel ćwiczenia\n",
        "\n",
        "Celem ćwiczenia jest nauczenie modelu z wykorzystaniem danych etykietowanych i nieetykietowanych ze zbioru STL10 z użyciem metody [Bootstrap your own latent](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "Metoda ta jest relatywnie \"lekka\" obliczeniowo, a także dość prosta do zrozumienia i zaimplementowania, dlatego też na niej się skupimy na tych laboratoriach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI8ZMEH2NkgA"
      },
      "source": [
        "# Zbiór STL10\n",
        "\n",
        "Zbiór STL10 to zbiór stworzony i udostępniony przez Stanford [[strona]](https://ai.stanford.edu/~acoates/stl10/) [[papier]](https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf) a inspirowany przez CIFAR-10. Obrazy zostały pozyskane z [ImageNet](https://image-net.org/). Szczegóły można doczytać na ich stronie. To co jest ważne to to, że autorzy zbioru dostarczają predefiniowany plan eksperymentalny, żeby móc porównywać łatwo wyniki eksperymentów. Nie będziemy go tutaj stosować z uwagi na jego czasochłonność (10 foldów), ale warto pamiętać o tym, że często są z góry ustalone sposoby walidacji zaprojetowanych przez nas algorytmów na określonych zbiorach referencyjnych.\n",
        "\n",
        "Korzystając z `torchvision.datasets` ***załaduj*** 3 podziały zbioru danych STL10: `train`, `test`, `unlabeled` oraz utwórz z nich instancje klasy `DataLoader`. Korzystając z Google Colab rozważ użycie Google Drive do przechowyania zbioru w calu zaoszczędzenia czasu na wielokrotne pobieranie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC8VhuEoR90S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qyXdlLZHzn"
      },
      "source": [
        "# Uczenie nadzorowane\n",
        "\n",
        "Żeby porównać czy metoda BYOL przynosi nam jakieś korzyści musimy wyznaczyć wartość bazową metryk(i) jakości, których będziemu używać (np. dokładność).\n",
        "\n",
        "***Zaimplementuj*** wybraną metodę uczenia nadzorowanego na danych `train` z STL10. Możesz wykorzystać predefiniowane architektury w `torchvision.models` oraz kody źródłowe z poprzednich list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2vcmEhEaA2a"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saaKpwl0FVII"
      },
      "source": [
        "# Bootstrap your own latent\n",
        "\n",
        "Metoda [Bootstrap your own latent](https://arxiv.org/abs/2006.07733) jest opisana w rodziale 3.1 papieru a także w dodatku A. Składa się z dwóch etapów:\n",
        "\n",
        "\n",
        "1.   uczenia samonadzorowanego (ang. *self-supervised*)\n",
        "2.   douczania nadzorowanego (ang. *fine-tuning*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8L_zYGNs_K"
      },
      "source": [
        "## Uczenie samonadzorowane\n",
        "\n",
        "Architektura do nauczania samonadzorowanego składa się z dwóch sieci: (1) *online* i (2) *target*. W uproszczeniu cała architektura działa tak:\n",
        "\n",
        "\n",
        "1.   Dla obrazka $x$ wygeneruj dwie różne augmentacje $v$ i $v'$ za pomocą funkcji $t$ i $t'$.\n",
        "2.   Widok $v$ przekazujemy do sieci *online*, a $v'$ do *target*.\n",
        "3.   Następnie widoki przekształacamy za pomocą sieci do uczenia reprezentacji (np. resnet18 lub resnet50) do reprezentacji $y_\\theta$ i $y'_\\xi$.\n",
        "4.   Potem dokonujemy projekcji tych reprezentacji w celu zmniejszenia wymiarowości (np. za pomocą sieci MLP).\n",
        "5.   Na sieci online dokonujmey dodatkowo predykcji pseudo-etykiety (ang. *pseudolabel*)\n",
        "6.   Wyliczamy fukncję kosztu: MSE z wyjścia predyktora sieci *online* oraz wyjścia projekcji sieci *target* \"przepuszczonej\" przez predyktor sieci *online* **bez propagacji wstecznej** (*vide Algorithm 1* z papieru).\n",
        "7.   Dokonujemy wstecznej propagacji **tylko** po sieci *online*.\n",
        "8.   Aktualizujemy wagi sieci *target* sumując w ważony sposób wagi obu sieci $\\xi = \\tau\\xi + (1 - \\tau)\\theta$ ($\\tau$ jest hiperprametrem) - jest to ruchoma średnia wykładnicza (ang. *moving exponential average*).\n",
        "\n",
        "Po zakończeniu procesu uczenia samonadzorowanego zostawiamy do douczania sieć kodera *online* $f_\\theta$. Cała sieć *target* oraz warstwy do projekcji i predykcji w sieci *online* są \"do wyrzucenia\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIRw--bTYe5"
      },
      "source": [
        "### Augmentacja\n",
        "\n",
        "Dodatek B publikacji opisuje augmentacje zastosowane w metodzie BYOL. Zwróć uwagę na tabelę 6 w publikacji. `torchvision.transforms.RandomApply` może być pomocne.\n",
        "\n",
        "***Zaimeplementuj*** augmentację $\\tau$ i $\\tau'$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csbR-Bvy8IbZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKMQGx8FtoF"
      },
      "source": [
        "### Implementacja uczenia samonadzorowanego\n",
        "\n",
        "***Zaprogramuj*** proces uczenia samonadzorowanego na danych nieetykietowanych ze zbioru STL10.\n",
        "\n",
        "Wskazówki do realizacji polecenia:\n",
        "\n",
        "1. Proces uczenia może trwać bardzo długo dlatego zaleca się zastsowanie wczesnego zatrzymania lub uczenia przez tylko jedną epokę. Mimo wszystko powinno się dać osiągnąć poprawę w uczeniu nadzorowanym wykorzystując tylko zasoby z Google Colab.\n",
        "2. Dobrze jest pominąć walidację na zbiorze treningowym i robić ją tylko na zbiorze walidacyjnym - zbiór treningowy jest ogromny i w związku z tym narzut czasowy na walidację też będzie duży.\n",
        "3. Walidację modelu można przeprowadzić na zbiorze `train` lub całkowicie ją pominąć, jeżeli uczymy na stałej ilości epok.\n",
        "4. Rozważ zastosowanie tylko jednej augmentacji - augmentacja $\\tau'$ jest bardziej czasochłonna niż $\\tau$.\n",
        "5. Poniżej jest zaprezentowany zalążek kodu - jest on jedynie wskazówką i można na swój sposób zaimplementować tę metodę"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5NarrwBiJIk"
      },
      "source": [
        "from copy import deepcopy\n",
        "from torch import nn\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    def __init__(self, model, labels_no, augmentation, augmentation_prim, encoder_out_shape = 1000, projection_size = 256, tau = 0.999):\n",
        "        super().__init__()\n",
        "        self.encoder_online = model\n",
        "        self.projector_online = mlp(encoder_out_shape, projection_size=projection_size)\n",
        "        self.online_common = nn.Sequential(self.encoder_online, self.projector_online)\n",
        "        self.predictor_online = nn.Linear(projection_size, labels_no)\n",
        "        self.online = nn.Sequential(self.online_common, self.predictor_online)\n",
        "\n",
        "        self.encoder_target = deepcopy(self.encoder_online)\n",
        "        self.projector_target = deepcopy(self.projector_online)\n",
        "        self.target = nn.Sequential(self.encoder_online, self.projector_online)\n",
        "\n",
        "        self.tau = tau\n",
        "        self.augmentation = augmentation\n",
        "        self.augmentation_prim = augmentation_prim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qitno0wc8W35"
      },
      "source": [
        "## Douczanie nadzorowane\n",
        "\n",
        "***Zaimplementuj*** proces douczania kodera z poprzedniego polecenia na danych etykietowanych ze zbioru treningowego. Porównaj jakość tego modelu z modelem nauczonym tylko na danych etykietownaych. Postaraj się wyjaśnić różnice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMQc6aZXgI_"
      },
      "source": [
        "state_dict = model.encoder_online.state_dict()\n",
        "encoder = ___\n",
        "encoder.load_state_dict(state_dict)\n",
        "\n",
        "___"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}