{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqipF2CL3d4z"
      },
      "source": [
        "# Wstęp\n",
        "\n",
        "Zadanie nr 13 wprowadza zagadnienie głębokich modeli uczenia ze wzmocnieniem (_Deep Reinforcement Learning_, **DRL**) - modeli, które uczone są przez interakcję z otoczeniem, obserwując konsekwencję działań oraz przyznawaną wartość nagrody.\n",
        "\n",
        "## Cel ćwiczenia\n",
        "\n",
        "Celem ćwiczenia jest zapoznanie z:\n",
        "\n",
        "- koncepcją głębokiego uczenia ze wzmocnieniem,\n",
        "- podstawowymi podejściami w głębokim uczeniu ze wzmocnieniem,\n",
        "- przykładowymi środowiskami, wykorzystywanym w DRL,\n",
        "- jedną z podstawowych metod DRL opartą na funkcji wartości.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-lqPDpVNDxQ"
      },
      "source": [
        "# Głębokie uczenie ze wzmocnieniem\n",
        "\n",
        "Podstawa reinforcement learning jest uczenie przez interakcje ̨: agent oddziałuje z otoczeniem i, obserwując konsekwencje jego działań, może uczyć się jak zmienić swoje zachowanie by zmaksymalizować wartość otrzymanej nagrody.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Zasada działania modelu uczenia ze wzmocnieniem jest następująca: autonomiczny agent, sterowany przez model, obserwuje **stan $s_t$ środowiska** w kroku czasowym $t$. Agent oddziałuje z otoczeniem podejmując **akcję $a_t$**, co powoduje przejście otoczenia i agenta do nowego stanu $s_{t+1}$. Przejście to odbywa się na podstawie **funkcji przejścia stanów $T(s_{t+1} | s_t, a_t)$** (równoważnej środowisku) - na podstawie aktualnego stanu i podjętej akcji. Zakładamy, że informacja o stanie jest wystarczająca dla agenta do podjęcia optymalnej akcji.\n",
        "\n",
        "Sekwencja akcji wybierana jest na podstawie nagrody, przyznawanej przez otoczenie agentowi. Każde przejście środowiska do nowego stanu powoduje nagrodzenie agenta **nagrodą $r_t$**, tj. wartością skalarną, określającą \"poprawność\" podjętej decyzji. Celem uczenia modelu DRL jest wyuczenie **strategii**, maksymalizującej łączną uzyskiwaną wartość nagrody (_zwrot_).\n",
        "\n",
        "Agent uczenia ze wzmocnieniem może składać się z następujących elementów (w zależności od stosowanego podejścia zawiera jeden lub więcej z nich):\n",
        "\n",
        "- **strategia** (ang. _policy_), tj. funkcja zachowywania się agenta; może być deterministyczna $a = \\pi(s)$ lub stochastyczna $\\pi(a|s)=p(a_t=a | s_t=s)$;\n",
        "- **funkcja wartości** (ang. _value function_), tj. funkcja służąca do predykcji przyszłej nagrody, używana do oceny stanów przy podejmowaniu akcji:\n",
        "  $$V_{\\pi}(s)=\\mathbb{E}\\left[r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+3}+\\dots | s_t=s\\right],$$\n",
        "  gdzie $\\gamma\\in[0, 1]$ to współczynnik dyskontowania, określający, w jakim stopniu faworyzowane są nagrody natychmiastowe;\n",
        "- **model**, służący do predykcji zmian w środowisku:\n",
        "  $$\\mathcal{P}_{ss'}^a = p(s_{t+1}=s | s_t=s, a_t=a),$$\n",
        "  $$\\mathcal{R}_s^a = \\mathbb{E}[r_{t+1} | s_t=s, a_t=a],$$\n",
        "  gdzie $\\mathcal{P}$ przewiduje kolejny krok, a $\\mathcal{R}$ wskazuje następną wartość nagrody.\n",
        "\n",
        "W zależności od elementów metody uczenia ze wzmocnieniem dzielimy na:\n",
        "\n",
        "- oparte na funkcji wartości (bez strategii) - trenujemy model aby poprawnie oceniał wartość nagrody za każdą z możliwych podejmowanych akcji,\n",
        "- oparte na strategii (bez funkcji wartości) - _policy gradient_ - trenujemy model strategii, która bezpośrednio zwraca akcje, maksymalizujące wartość nagrody,\n",
        "- aktor-krytyk (wykorzystujące strategię oraz funkcję wartości) - połączenie powyższych (aktor to model strategii, kontrolujący zachowanie agenta, a krytyk to model funkcji wartości, oceniająca akcje podejmowane przez krytyka).\n",
        "\n",
        "Każda z tych metod może być też _model-free_ (bez wykorzystania modelu środowiska) lub _model-based_ (modelująca środowisko).\n",
        "\n",
        "W tym zadaniu skupimy się na **podejściu _model-free_ opartym na funkcji wartości**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJHP9jlwyaKf"
      },
      "source": [
        "## Funkcje wartości\n",
        "\n",
        "Podejścia wykorzystujące funkcje wartości to metody oparte na estymowaniu wartości zwrotu (całkowitej nagrody). Wartość ta może być estymowana wyłącznie na podstawie aktualnego stanu - wówczas mówimy o **funkcji stan-wartość** $V^\\pi(s) = \\mathbb{E}[R | s, \\pi]$, która ocenia wartość nagrody przy założeniu postępowania od danego stanu zgodnie ze strategią $\\pi$. W idealnej sytuacji, optymalna strategia mogłaby być wybierana na podstawie akcji maksymalizujących oczekiwaną wartość nagrody; niestety jednak funkcja przejścia stanów $T(s_{t+1} | s_t, a_t)$ nie jest znana agentowi.\n",
        "\n",
        "Rozwiązaniem jest konstrukcja funkcji, która będzie estymowała wartość zwrot na podstawie stanu oraz podejmowanej akcji - **funkcja jakości** $Q^\\pi(s, a) = \\mathbb{E}[R | s, a, \\pi]$ - tu oceniana jest wartość zwrotu po podjęciu danej akcji w danym stanie, przy postępowaniu dalej zgodnie ze strategią $\\pi$. Wówczas, najlepsza strategia może być znaleziona przez zachłanny wybór akcji maksymalizującej wartość funkcji $Q$: $\\arg\\max_a Q^\\pi(s, a)$, w ten sposób definiując aproksymację poszukiwanej funkcji wartości na podstawie maksymalizowania funkcji jakości:\n",
        "\n",
        "$$V^\\pi(s) \\approx \\max_a Q^\\pi(s, a).$$\n",
        "\n",
        "W takim podejściu skupiamy się na eksploatacji aktualnie wykrytego maksimum; często jednak chcielibyśmy również eksplorować rozwiązania alternatywne, na co pozwala strategia $\\epsilon$-zachłanna (z prawdopodobieństwem $1-\\epsilon$ wybieramy maksymalizującą akcję, a z prawdopodobieństwem $\\epsilon$ - losową).\n",
        "\n",
        "Wartość funkcji jakości może być obliczona ze wzoru rekurencyjnego:\n",
        "\n",
        "$$Q^\\pi(s_t, a_t) = \\mathbb{E}_{s_{t+1}}\\left[r_t + \\gamma Q^\\pi\\left(s_{t+1}, \\pi(s_{t+1})\\right)\\right],$$\n",
        "\n",
        "mówimy wówczas o podejściu _on-policy_ - do estymacji całkowitej wartości zwrotu i wyboru akcji wykorzystuje się aktualną postać strategii (np. algorytm [SARSA](https://en.wikipedia.org/wiki/State–action–reward–state–action)). Alternatywnie, wybór akcji i estymacja wartości zwrotu może odbywać się poprzez zachłanny wybór akcji maksymalizującej wartość nagrody - mówimy wtedy o algorytmach _off-policy_:\n",
        "\n",
        "$$Q^\\pi(s_t, a_t) = \\mathbb{E}_{s_{t+1}}\\left[r_t + \\gamma \\max_a Q^\\pi\\left(s_{t+1}, a_{t+1}\\right)\\right].$$\n",
        "\n",
        "Przykładem drugiej z grup metod jest [Q-learning](https://en.wikipedia.org/wiki/Q-learning). Polega on na budowaniu tabeli pamięci $Q[s, a]$, mającej przechowywać wartości funkcji jakości dla wszystkich możliwych kombinacji stanów $s$ i akcji $a$. Powstaje ona na skutek iteracyjnego próbkowania akcji z bieżącego stanu i zapisywania wartości zwrotu ($Y$), przy dalszym podejmowaniu akcji maksymalizujących wartość uzyskiwanej nagrody. Algorytm powtarzany jest iteracyjnie dla wszystkich możliwych akcji w danym stanie, po czym następuje przejście do kolejnego, niezbadanego stanu. W wytrenowanym modelu tabela ta wykorzystywana jest do wyboru akcji, maksymalizując wartość uzyskiwanej funkcji nagrody.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N2A6IzkFL1H"
      },
      "source": [
        "## Deep Q-Learning (DQL)\n",
        "\n",
        "Algorytm Q-learning jest słabo skalowalny — w szczególności pod względem wymagań pamięciowych oraz złożoności obliczeniowej dla dużych przestrzeni stanów i akcji, dlatego też w DQN tabela pamięci została zastąpiona przez sieć głęboką (ang. _Deep Q-Network_), której zadaniem jest aproksymacja funkcji jakości $Q(s, a)$.\n",
        "\n",
        "Uczenie tej sieci odbywa się np. z użyciem funkcji celu - błędu średniokwadratowego między predykcją sieci a końcowymi wartościami zwrotu\n",
        "\n",
        "$$\\mathcal{L} = \\mathbb{E}\\left[\\left(r_t + \\gamma \\max_a Q^\\pi\\left(s_{t+1}, a_{t+1}\\right) - Q^\\pi\\left(s_t, a_t\\right)\\right)^2\\right].$$\n",
        "\n",
        "Można także użyć [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).\n",
        "\n",
        "Algorytm uczenia tej sieci przedstawiony został poniżej:\n",
        "\n",
        "```{tidy=FALSE, eval=FALSE}\n",
        "Get(s);                                         // pobierz stan początkowy\n",
        "for step <- 1, 2, ... till convergence do       // dla kolejnych kroków\n",
        "    Sample(a);                                  // wybierz akcję (epsilon-zachłannie)\n",
        "    Get(s');                                    // ustal nowy stan\n",
        "    if s' is terminal then                      // jeżeli nowy stan jest stanem końcowym\n",
        "        Y <- sum(r);                            // ustal wartość zwrotu\n",
        "        Sample(new s');                         // pobierz nowy stan początkowy\n",
        "    else:\n",
        "        Y <- r + gamma * Q_step(s', a')'        // aktualizuj estymację zwrotu\n",
        "    theta_k+1 <- theta - alpha * grad L(theta); // aktualizuj wagi zgodnie z funkcją celu\n",
        "    s <- s';                                    // weź nowy stan jako aktualny\n",
        "```\n",
        "\n",
        "Wybrane ulepszenia:\n",
        "\n",
        "- **Zmienna wartość $\\epsilon$**.\n",
        "  Zastosowanie stałej wartości $\\epsilon$ w strategii $\\epsilon$-zachłannej nie jest najlepszym rozwiązaniem. Zwykle wolelibyśmy, żeby w początkowym etapie model bardziej eksplorował przestrzeń (wyższy $\\epsilon$), a w późniejszym etapie by eksploatował znalezione optimum (niższy $\\epsilon$). Często stosuje się w tym celu funkcję wykładniczą.\n",
        "\n",
        "- **_Target network_**.\n",
        "  Stosowane w celu poprawy stabilności trenowania. Polega ono na wykorzystaniu odrębnych instancji do działania w środowisku (_target network_) i odrębnej do trenowania w każdej iteracji (_policy network_). Początkowo, obie sieci są swoimi kopiami, a ich wagi są synchronizowane w trakcie trenowania w określonych interwałach. Algorytm trenowania modelu z _target network_ prezentuje się następująco:\n",
        "\n",
        "  1. Wykonaj kopię modelu (_target network_)\n",
        "  1. Dla każdego kroku:\n",
        "     - wykonuj akcję w środowisku z użyciem _policy network_ i zapisz czwórkę [stan, akcja, następny stan, nagroda] do utworzenia mini-batcha\n",
        "     - oblicz oczekiwane wartości funkcji Q używając _target network_\n",
        "     - oblicz loss i wykonaj propagację wsteczną w _policy network_\n",
        "  1. Cyklicznie synchronizuj wytrenowane wagi w _policy network_ -> _target network_\n",
        "\n",
        "- **_Experience replay_**.\n",
        "  Podobnie jak _target network_ stosowane w celu poprawy stabilności trenowania. Polega ono na zapisywaniu czwórek: stan, akcja, kolejny stan i nagroda $(s_t, a_t, s_{t+1}, r_t)$ w pamięci o ograniczonej wielkości (_experience_). W celu uczenia modelu próbkujemy losowo mini-batch z _experience_ i wykorzystujemy do trenowania. W ten sposób model uczony jest na danych niezależnych od siebie, poprawiając stabilność trenowania.\n",
        "\n",
        "  Wykorzystując _experience replay_ algorytm trenowania będzie wyglądał następująco:\n",
        "\n",
        "  1. Zbuduj pamięć _experience_ wykonując akcje niewytrenowanym modelem w środowisku i zapisując czwórki.\n",
        "  1. Dla każdego kroku uczenia:\n",
        "     - wykonaj akcję (1 lub więcej) w środowisku i zapisz w pamięci, usuwając najstarszy experience\n",
        "     - próbkuj batch z experience losowo i wykorzystaj do trenowania modelu (trenowanie odbywa się analogicznie jak wcześniej)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbN3BpNsvpOL"
      },
      "source": [
        "### Uwaga ogólna\n",
        "\n",
        "Modele DRL, szczególnie w najbardziej \"podstawowych\" formach są niestabilne przy trenowaniu, dlatego też stosuje się wiele ulepszeń. Ponadto każdy błąd implementacyjny może mieć bardzo duży wpływ na zdolność modelu do wytrenowania, dlatego też w tych listach będziemy się opierać na gotowych implementacjach z biblioteki [**Stable-Baselines3**](https://stable-baselines.readthedocs.io). Opiera się ona na PyTorchu i zawiera sprawdzone i przetestowane implementacje wielu popularnych metod głębokiego uczenia ze wzmocnieniem. W tym laboratorium skupimy się na wspomnianej wcześniej metodzie Deep Q-Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh2_lpWIXq78"
      },
      "source": [
        "## Środowisko\n",
        "\n",
        "W laboratorium będziemy wykorzystali bibliotekę [Gym](https://gym.openai.com) - zestaw narzędzi do budowy i porównywania algorytmów uczenia ze wzmocnieniem. Zawiera ona środowiska o różnym poziomie złożoności - zaczniemy od prostego środowiska - [CartPole](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "> oparte na: Barto, Sutton & Anderson _Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem_, 1983\n",
        "\n",
        "Słupek zamocowany jest do wózka, który porusza się bez tarcia po ścieżce. Przestrzeń stanów jest czteroelementowa:\n",
        "\n",
        "- pozycja wózka,\n",
        "- szybkość wózka,\n",
        "- kąt nachylenia słupka,\n",
        "- szybkość obrotu słupka.\n",
        "\n",
        "Możemy kontrolować ruch wózka przykładając do niego siłę +1 lub -1. Celem jest zapobieganie przewróceniu słupka. Nagroda +1 przyznawana jest za każdy krok, kiedy słupek pozostaje pionowo. Epizod kończy się, gdy słupek odchylony jest o więcej niż 15 stopni od pionu, lub gry wózek odsunie się o ponad 2,4 jednostki od środka.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C7ngYwDJTlJL"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb > /dev/null 2>&1\n",
        "!apt-get install python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay stable-baselines3[extra] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0SeMUHF3SSq",
        "outputId": "827cd8ee-f5f7-4352-ca67-20876280afba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: /opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin: No such file or directory\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7998a15431f0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!$PATH\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import animation\n",
        "from pyvirtualdisplay import Display\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import plot_results, load_results\n",
        "from stable_baselines3.dqn.policies import MlpPolicy\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "SEED = 13\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "DISPLAY = Display(visible=0, size=(400, 300))\n",
        "DISPLAY.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g33fPfh3Dzv"
      },
      "source": [
        "Metoda Deep Q-Learning wraz z wymienionymi wcześniej ulepszeniami jest już zaimplementowana w bibliotece Stable-Baselines3. Najpierw sprawdźmy hiperparametry, jakie możemy podać do modelu:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_1b0HTmikQp"
      },
      "outputs": [],
      "source": [
        "help(DQN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHyCoZfTm8g_"
      },
      "source": [
        "Kluczowym argumentem podawanym do modelu jest `policy`, czyli model strategii wykorzystywany w modelu. W Stable-Baselines3 każda metoda DRL posiada własny zestaw strategii - w przypadku DQN dostępne są trzy wersje (moduł `stable_baselines3.dqn.policies`:\n",
        "\n",
        "- `MlpPolicy` - w pełni połączona\n",
        "- `CnnPolicy` - z warstwami konwolucyjnymi\n",
        "- `MultiInputPolicy` - dla środowisk, które umożliwiają obserwacje w formie obrazów jak i wartości.\n",
        "\n",
        "W tym przykładzie będziemy wykorzystywać `MlpPolicy`, ponieważ środowisko nie zwraca obserwacji w formie obrazów. Sprawdźmy zatem, jakie argumenty ona przyjmuje (mogą one być przekazywane przez argument modelu `policy_kwargs`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C8ErEIom7zP"
      },
      "outputs": [],
      "source": [
        "help(MlpPolicy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQ7GdY0pGH_"
      },
      "source": [
        "Szczególnie interesującym będzie dla nas argument `net_arch`, definiujący architekturę modelu strategii (jest to lista integerów, opisująca liczbę neuronów w poszczególnych warstwach ukrytych).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzQunlqOmV5M"
      },
      "source": [
        "Deklarujemy funkcję do wyświetlania działania modelu. Poza tworzeniem animacji wykonuje ona również ewaluację modelu, sprawdzając w 100 epizodach osiąganą całkowitą wartość zwrotu.\n",
        "\n",
        "Sprawdzimy od razu, jak zachowuje się niewytrenowany (losowy) model (zwróćmy uwagę, że musimy podać model strategii, jak i środowisko, w jakim model ma działać - możemy uzyskać do niego dostęp wywołując metodę modelu `get_env`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "677nVrUHiioW"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, max_steps=50):\n",
        "    \"\"\"Show how a model works in the environment.\"\"\"\n",
        "    env = model.get_env()\n",
        "    obs = env.reset()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    frames = []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "        with torch.no_grad():\n",
        "            action, _ = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if len(frames) >= max_steps:\n",
        "            break\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig=fig,\n",
        "        func=lambda i: ax.imshow(frames[i]),\n",
        "        frames=range(len(frames)),\n",
        "        interval=100,\n",
        "        blit=False,\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "    reward_avg, reward_std = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "    print(f\"Reward in episode: {reward_avg:.2f} +/- {reward_std:.2f}\")\n",
        "\n",
        "    return anim.to_jshtml()\n",
        "\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "model = DQN(MlpPolicy, env)\n",
        "display(ipythondisplay.HTML(visualize_model(model)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUtoMG7sqTy_"
      },
      "source": [
        "Zaczniemy od deklaracji uproszczonego modelu, ze stałą wartością $\\epsilon$, bez target network oraz z experience replay o długości batcha (efektywnie - brakiem experience replay). Następnie będziemy chcieli włączać ulepszenia oraz stroić hiperparametry aby sprawdzić, jak wpływają one na działanie modelu. Do wytrenowania modelu służy metoda `learn`. Sprawdźmy jej parametry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHj0tvl3rsH6"
      },
      "outputs": [],
      "source": [
        "help(DQN.learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-xD5_RtsEzU"
      },
      "source": [
        "Z punktu widzenia analizy działania modelu potrzebna jest jedynie liczba kroków uczenia `total_timesteps`. Możemy też wyregulować częstość ewaluacji oraz logowania (wyświetlanie logu odbywa się jedynie przy ustawieniu `verbose=1`). Oprócz tego chcielibyśmy jednak także wyświetlić krzywą uczenia. Wykorzystujemy w tym celu wrapper na środowisko w Unity `Monitor`. Jako parametry przyjmuje on środowisko, które będziemy monitorować, oraz folder, w którym będziemy zapisywać logi. Możemy je potem wczytać i wyświetlić wykorzystując metody `load_results` i `plot_results`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SEv3tDAcxN56"
      },
      "outputs": [],
      "source": [
        "RESULTS_DIR = \"/tmp/\"\n",
        "\n",
        "env = Monitor(env, filename=RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rgc6dZwzpKl"
      },
      "source": [
        "Deklarujemy model. Ustawiając rozmiar bufora oraz start i częstość trenowania równe wielkości mini-batcha efektywnie wyłączamy experience replay, a aktualizując sieć w interwale równym wielkości batcha model trenowany będzie bez _target network_. Ponadto, dobieramy odpowiednio wartości schedulera $\\epsilon$, tak, aby współczynnik ten był stały podczas trenowania.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KBOm4cxqS4s"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "gamma = 0.8\n",
        "eps = 0.1\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "log_dir = \"/tmp/vanilla/\"\n",
        "os.makedirs(log_dir, exist_ok=True)  # we have to create this dir - it does not exist\n",
        "env = Monitor(env, filename=log_dir)\n",
        "\n",
        "vanilla_dqn = DQN(\n",
        "    MlpPolicy, env,\n",
        "    learning_rate=lr, batch_size=batch_size, gamma=gamma, seed=SEED,\n",
        "    buffer_size=batch_size, learning_starts=batch_size, train_freq=batch_size,  # disable experience replay\n",
        "    target_update_interval=batch_size,  # disable target network\n",
        "    exploration_fraction=1, exploration_initial_eps=eps, exploration_final_eps=eps,  # disable epsilon scheduling\n",
        "    verbose=1,  # show information throughout training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu9tlouWxE_o"
      },
      "source": [
        "Wytrenujmy zatem taki prosty model w 100 000 kroków i sprawdźmy jakie osiągnie on po tym czasie rezultaty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzaBRZ0tsaXs"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 100000\n",
        "\n",
        "vanilla_dqn.learn(total_timesteps)\n",
        "display(ipythondisplay.HTML(visualize_model(vanilla_dqn)))\n",
        "plot_results([log_dir], total_timesteps, \"timesteps\", \"Vanilla DQN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0jmTJ5SwEIG"
      },
      "source": [
        "Jak widać, metoda Deep Q-Learning w domyślnym ustawieniu nie działa zbyt dobrze - model nie zbiega, oscylując wokół początkowych wartości nagrody. Być może strojenie hiperparametrów poprawiłoby osiągane rezultaty?\n",
        "\n",
        "Sprawdźmy, jaka była maksymalna wartość zgromadzona w trakcie uczenia. Posłuży nam w tym celu funkcja `load_results`. Zwraca ona pandasowy DataFrame, w którym znajdują się informacje o każdym epizodzie w trakcie trenowania: uzyskanej nagrodzie `r`, długości epizodu `l` oraz jego czasie trwania `t`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au1bq1VW_dKz"
      },
      "outputs": [],
      "source": [
        "results = load_results(log_dir)\n",
        "display(results.head(5))\n",
        "print(f\"Max reward: {results.r.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exy8sgW_AcQ2"
      },
      "source": [
        "Spróbujemy powtórzyć wytrenowanie modelu, tym razem z wykorzystaniem _experience replay_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkA_isrUwDYs"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "gamma = 0.8\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "log_dir = \"/tmp/exp_replay/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = Monitor(env, filename=log_dir)\n",
        "\n",
        "dqn_exp = DQN(\n",
        "    MlpPolicy, env,\n",
        "    learning_rate=lr, batch_size=batch_size, gamma=gamma, seed=SEED,\n",
        "    target_update_interval=batch_size,  # disable target network\n",
        "    verbose=1,  # show information throughout training\n",
        ")\n",
        "\n",
        "total_timesteps = 100000\n",
        "\n",
        "dqn_exp.learn(total_timesteps)\n",
        "display(ipythondisplay.HTML(visualize_model(dqn_exp)))\n",
        "plot_results([log_dir], total_timesteps, \"timesteps\", \"DQN with experience replay\")\n",
        "results = load_results(log_dir)\n",
        "print(f\"Max reward: {results.r.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWYSR5JTFJMz"
      },
      "source": [
        "Jak widać rezultaty są dużo lepsze. Ale może udałoby się poprawić proces trenowania? Tego dotyczyć będą zadania.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ppomZQy2R6"
      },
      "source": [
        "## Zadanie 1\n",
        "\n",
        "Zbadaj wpływ pozostałych ze wspomnianych hipeparametrów na trenowanie i działanie modelu. Skup się na zmianie architektury modelu strategii, hiperparametrach trenowania (gamma, epsilon itp.). Zapisz swoje wnioski oraz przygotuj wizualizacje uzyskiwanych rezultatów oraz animacje działania modelu.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daT_PtfSa_Yg"
      },
      "source": [
        "## Zadanie 2\n",
        "\n",
        "Wykorzystaj metodę Deep Q-Learning do przygotowania agenta działającego w wybranym innym środowisku z biblioteki [Gym](https://gym.openai.com/envs). Ze względu na cechy modelu zwróć uwagę na dopasowanie środowiska (np. przestrzeń akcji powinna być dyskretna). Zapoznaj się z definicją funkcji nagrody przyznawanej agentowi w środowisku, aby można było zinterpretować rezultaty osiągane przez model. Jeśli zdecydujesz się na wykorzystanie środowiska z obserwacjami wizualnymi, pamiętaj o zmianie typu modelu strategii (uwaga: trenowanie modeli w oparciu o obserwacje wizualne z wykorzystaniem sieci konwolucyjnym jest długotrwałe i może być bardziej niestabilne niż w przypadku strategii MLP). Spróbuj dostroić hiperparametry trenowania do nowego środowiska.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ1P-ydIeLO4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
