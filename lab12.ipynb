{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRya3-1t7gO7"
      },
      "source": [
        "# Wstęp\n",
        "\n",
        "Metody uczenia maszynowego możemy podzielić na dwie główne kategorie (pomijając uczenie ze wzmocnieniem): nadzorowane i nienadzorowane. Uczenie **nadzorowane** (ang. *supervised*) to jest uczenie z dostępnymi etykietami dla danych wejściowych. Na parach danych uczących $dataset= \\{(x_0,y_0), (x_1,y_1), \\ldots, (x_n,y_n)\\}$ model ma za zadanie nauczyć się funkcji $f: X \\rightarrow Y$. Z kolei modele uczone w sposób **nienadzorowany** (ang. *unsupervised*) wykorzystują podczas trenowania dane nieetykietowane tzn. nie znamy $y$ z pary $(x, y)$.\n",
        "\n",
        "Dość częstą sytuacją, z jaką mamy do czynienia, jest posiadanie małego podziobioru danych etykietowanych i dużego nieetykietowanych. Często annotacja danych wymaga ingerencji człowieka - ktoś musi określić co jest na obrazku, ktoś musi powiedzieć czy dane słowo jest rzeczownkiem czy czasownikiem itd.\n",
        "\n",
        "Jeżeli mamy dane etykietowane do zadania uczenia nadzorowanego (np. klasyfikacja obrazka), ale także dużą ilość danych nieetykietowanych, to możemy wtedy zastosować techniki **uczenia częściowo nadzorowanego** (ang. *semi-supervised learning*). Te techniki najczęściej uczą się funkcji $f: X \\rightarrow Y$, ale jednocześnie są w stanie wykorzystać informacje z danych nieetykietowanych do poprawienia działania modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjjlvGdZNg00"
      },
      "source": [
        "## Cel ćwiczenia\n",
        "\n",
        "Celem ćwiczenia jest nauczenie modelu z wykorzystaniem danych etykietowanych i nieetykietowanych ze zbioru STL10 z użyciem metody [Bootstrap your own latent](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "Metoda ta jest relatywnie \"lekka\" obliczeniowo, a także dość prosta do zrozumienia i zaimplementowania, dlatego też na niej się skupimy na tych laboratoriach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI8ZMEH2NkgA"
      },
      "source": [
        "# Zbiór STL10\n",
        "\n",
        "Zbiór STL10 to zbiór stworzony i udostępniony przez Stanford [[strona]](https://ai.stanford.edu/~acoates/stl10/) [[papier]](https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf) a inspirowany przez CIFAR-10. Obrazy zostały pozyskane z [ImageNet](https://image-net.org/). Szczegóły można doczytać na ich stronie. To co jest ważne to to, że autorzy zbioru dostarczają predefiniowany plan eksperymentalny, żeby móc porównywać łatwo wyniki eksperymentów. Nie będziemy go tutaj stosować z uwagi na jego czasochłonność (10 foldów), ale warto pamiętać o tym, że często są z góry ustalone sposoby walidacji zaprojetowanych przez nas algorytmów na określonych zbiorach referencyjnych.\n",
        "\n",
        "Korzystając z `torchvision.datasets` ***załaduj*** 3 podziały zbioru danych STL10: `train`, `test`, `unlabeled` oraz utwórz z nich instancje klasy `DataLoader`. Korzystając z Google Colab rozważ użycie Google Drive do przechowyania zbioru w calu zaoszczędzenia czasu na wielokrotne pobieranie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hC8VhuEoR90S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2640397119/2640397119 [08:54<00:00, 4943938.48it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = torchvision.datasets.STL10(\n",
        "    root=\"./data\",\n",
        "    split=\"train\",\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "test_data = torchvision.datasets.STL10(\n",
        "    root=\"./data\",\n",
        "    split=\"test\",\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "unlabelled_data = torchvision.datasets.STL10(\n",
        "    root=\"./data\",\n",
        "    split=\"unlabeled\",\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "unlabelled_loader = DataLoader(unlabelled_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 5, 1, ..., 1, 7, 5], dtype=uint8)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qyXdlLZHzn"
      },
      "source": [
        "# Uczenie nadzorowane\n",
        "\n",
        "Żeby porównać czy metoda BYOL przynosi nam jakieś korzyści musimy wyznaczyć wartość bazową metryk(i) jakości, których będziemu używać (np. dokładność).\n",
        "\n",
        "***Zaimplementuj*** wybraną metodę uczenia nadzorowanego na danych `train` z STL10. Możesz wykorzystać predefiniowane architektury w `torchvision.models` oraz kody źródłowe z poprzednich list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l2vcmEhEaA2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 2.017242366754556\n",
            "Epoch 2/10, Loss: 1.672444363183613\n",
            "Epoch 3/10, Loss: 1.5604871586908269\n",
            "Epoch 4/10, Loss: 1.4099149190926854\n",
            "Epoch 5/10, Loss: 1.3230995347228232\n",
            "Epoch 6/10, Loss: 1.208315355868279\n",
            "Epoch 7/10, Loss: 1.0917893180364295\n",
            "Epoch 8/10, Loss: 0.9979507606240767\n",
            "Epoch 9/10, Loss: 0.8492815622800514\n",
            "Epoch 10/10, Loss: 0.779768540889402\n",
            "Accuracy: 0.575375\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import alexnet, AlexNet, AlexNet_Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load the pre-trained model\n",
        "model = alexnet(weights=AlexNet_Weights.DEFAULT)\n",
        "\n",
        "# Modify the final layer to match the number of classes in STL-10 (10 classes)\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: AlexNet,\n",
        "    train_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    num_epochs: int = 10,\n",
        "):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            # print(images)\n",
        "            # print(labels)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "\n",
        "def evaluate(model: AlexNet, test_loader: DataLoader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "def evaluate_classification_report(model: AlexNet, test_loader: DataLoader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=test_data.classes))\n",
        "\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.76      0.80      0.78       800\n",
            "        bird       0.54      0.52      0.53       800\n",
            "         car       0.88      0.80      0.84       800\n",
            "         cat       0.37      0.43      0.40       800\n",
            "        deer       0.59      0.58      0.59       800\n",
            "         dog       0.30      0.17      0.22       800\n",
            "       horse       0.78      0.40      0.52       800\n",
            "      monkey       0.36      0.76      0.49       800\n",
            "        ship       0.92      0.57      0.70       800\n",
            "       truck       0.65      0.74      0.69       800\n",
            "\n",
            "    accuracy                           0.58      8000\n",
            "   macro avg       0.62      0.58      0.58      8000\n",
            "weighted avg       0.62      0.58      0.58      8000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_classification_report(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8000\n",
            "['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
            "torch.Size([3, 96, 96])\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(test_data))\n",
        "print(test_loader.dataset.classes)\n",
        "print(test_data[0][0].size())\n",
        "print(test_data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saaKpwl0FVII"
      },
      "source": [
        "# Bootstrap your own latent\n",
        "\n",
        "Metoda [Bootstrap your own latent](https://arxiv.org/abs/2006.07733) jest opisana w rodziale 3.1 papieru a także w dodatku A. Składa się z dwóch etapów:\n",
        "\n",
        "\n",
        "1.   uczenia samonadzorowanego (ang. *self-supervised*)\n",
        "2.   douczania nadzorowanego (ang. *fine-tuning*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8L_zYGNs_K"
      },
      "source": [
        "## Uczenie samonadzorowane\n",
        "\n",
        "Architektura do nauczania samonadzorowanego składa się z dwóch sieci: (1) *online* i (2) *target*. W uproszczeniu cała architektura działa tak:\n",
        "\n",
        "\n",
        "1.   Dla obrazka $x$ wygeneruj dwie różne augmentacje $v$ i $v'$ za pomocą funkcji $t$ i $t'$.\n",
        "2.   Widok $v$ przekazujemy do sieci *online*, a $v'$ do *target*.\n",
        "3.   Następnie widoki przekształacamy za pomocą sieci do uczenia reprezentacji (np. resnet18 lub resnet50) do reprezentacji $y_\\theta$ i $y'_\\xi$.\n",
        "4.   Potem dokonujemy projekcji tych reprezentacji w celu zmniejszenia wymiarowości (np. za pomocą sieci MLP).\n",
        "5.   Na sieci online dokonujmey dodatkowo predykcji pseudo-etykiety (ang. *pseudolabel*)\n",
        "6.   Wyliczamy fukncję kosztu: MSE z wyjścia predyktora sieci *online* oraz wyjścia projekcji sieci *target* \"przepuszczonej\" przez predyktor sieci *online* **bez propagacji wstecznej** (*vide Algorithm 1* z papieru).\n",
        "7.   Dokonujemy wstecznej propagacji **tylko** po sieci *online*.\n",
        "8.   Aktualizujemy wagi sieci *target* sumując w ważony sposób wagi obu sieci $\\xi = \\tau\\xi + (1 - \\tau)\\theta$ ($\\tau$ jest hiperprametrem) - jest to ruchoma średnia wykładnicza (ang. *moving exponential average*).\n",
        "\n",
        "Po zakończeniu procesu uczenia samonadzorowanego zostawiamy do douczania sieć kodera *online* $f_\\theta$. Cała sieć *target* oraz warstwy do projekcji i predykcji w sieci *online* są \"do wyrzucenia\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIRw--bTYe5"
      },
      "source": [
        "### Augmentacja\n",
        "\n",
        "Dodatek B publikacji opisuje augmentacje zastosowane w metodzie BYOL. Zwróć uwagę na tabelę 6 w publikacji. `torchvision.transforms.RandomApply` może być pomocne.\n",
        "\n",
        "***Zaimeplementuj*** augmentację $\\tau$ i $\\tau'$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "csbR-Bvy8IbZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch import nn\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Parameter T T′\n",
        "# Random crop probability 1.0 1.0 x\n",
        "# Flip probability 0.5 0.5 x\n",
        "# Color jittering probability 0.8 0.8 x\n",
        "\n",
        "# Brightness adjustment max intensity 0.4 0.4\n",
        "# Contrast adjustment max intensity 0.4 0.4\n",
        "# Saturation adjustment max intensity 0.2 0.2\n",
        "# Hue adjustment max intensity 0.1 0.1\n",
        "\n",
        "# Color dropping probability 0.2 0.2 x\n",
        "# Gaussian blurring probability 1.0 0.1 x\n",
        "# Solarization probability 0.0 0.2\n",
        "\n",
        "\n",
        "def get_t1_aug():\n",
        "    transform = T.Compose(\n",
        "        [\n",
        "            T.RandomResizedCrop(size=(96, 96)),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            RandomApply(\n",
        "                T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n",
        "                p=0.8,\n",
        "            ),\n",
        "            RandomApply(T.Grayscale(num_output_channels=3), p=0.2),\n",
        "            T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)),\n",
        "        ]\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "\n",
        "def get_t2_aug():\n",
        "    transform = T.Compose(\n",
        "        [\n",
        "            T.RandomResizedCrop(size=(94, 94)),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            RandomApply(\n",
        "                T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n",
        "                p=0.8,\n",
        "            ),\n",
        "            RandomApply(T.Grayscale(num_output_channels=3), p=0.2),\n",
        "            RandomApply(T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)), p=0.1),\n",
        "            RandomApply(F.solarize, p=0.2),\n",
        "        ]\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn: nn.Module, p: float):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKMQGx8FtoF"
      },
      "source": [
        "### Implementacja uczenia samonadzorowanego\n",
        "\n",
        "***Zaprogramuj*** proces uczenia samonadzorowanego na danych nieetykietowanych ze zbioru STL10.\n",
        "\n",
        "Wskazówki do realizacji polecenia:\n",
        "\n",
        "1. Proces uczenia może trwać bardzo długo dlatego zaleca się zastsowanie wczesnego zatrzymania lub uczenia przez tylko jedną epokę. Mimo wszystko powinno się dać osiągnąć poprawę w uczeniu nadzorowanym wykorzystując tylko zasoby z Google Colab.\n",
        "2. Dobrze jest pominąć walidację na zbiorze treningowym i robić ją tylko na zbiorze walidacyjnym - zbiór treningowy jest ogromny i w związku z tym narzut czasowy na walidację też będzie duży.\n",
        "3. Walidację modelu można przeprowadzić na zbiorze `train` lub całkowicie ją pominąć, jeżeli uczymy na stałej ilości epok.\n",
        "4. Rozważ zastosowanie tylko jednej augmentacji - augmentacja $\\tau'$ jest bardziej czasochłonna niż $\\tau$.\n",
        "5. Poniżej jest zaprezentowany zalążek kodu - jest on jedynie wskazówką i można na swój sposób zaimplementować tę metodę"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch import Tensor\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
        "\n",
        "\n",
        "class LinearProbingClassifier:\n",
        "    def __init__(self, out_channels: int, **kwargs: Any) -> None:\n",
        "        self.out_channels = out_channels\n",
        "        self.model = LogisticRegression(solver=\"liblinear\", **kwargs)\n",
        "\n",
        "        self._z_train: list[Tensor] = []\n",
        "        self._y_train: list[Tensor] = []\n",
        "        self._z_test: list[Tensor] = []\n",
        "        self._y_test: list[Tensor] = []\n",
        "\n",
        "    @property\n",
        "    def z_train(self) -> Tensor:\n",
        "        return torch.cat(self._z_train, dim=0)\n",
        "\n",
        "    @property\n",
        "    def y_train(self) -> Tensor:\n",
        "        return torch.cat(self._y_train, dim=0)\n",
        "\n",
        "    @property\n",
        "    def z_test(self) -> Tensor:\n",
        "        return torch.cat(self._z_test, dim=0)\n",
        "\n",
        "    @property\n",
        "    def y_test(self) -> Tensor:\n",
        "        return torch.cat(self._y_test, dim=0)\n",
        "\n",
        "    def update_train(self, z: Tensor, y: Tensor) -> None:\n",
        "        self._z_train.append(z.detach().cpu())\n",
        "        self._y_train.append(y.detach().cpu())\n",
        "\n",
        "    def update_test(self, z: Tensor, y: Tensor) -> None:\n",
        "        self._z_test.append(z.detach().cpu())\n",
        "        self._y_test.append(y.detach().cpu())\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self._z_train = []\n",
        "        self._y_train = []\n",
        "        self._z_test = []\n",
        "        self._y_test = []\n",
        "\n",
        "    def fit(self) -> None:\n",
        "        self.model.fit(self.z_train, self.y_train)\n",
        "\n",
        "    def score(self, metric_prefix: str = \"\") -> dict[str, Tensor]:\n",
        "        y_score = torch.tensor(self.model.predict_proba(self.z_test))\n",
        "\n",
        "        metrics = self._get_metrics(metric_prefix)\n",
        "        metric_vals = metrics(y_score, self.y_test)\n",
        "\n",
        "        return metric_vals\n",
        "\n",
        "    def _get_metrics(self, metric_prefix: str) -> MetricCollection:\n",
        "        assert self.out_channels > 1\n",
        "        metrics = MetricCollection(\n",
        "            {\n",
        "                \"accuracy\": MulticlassAccuracy(num_classes=self.out_channels),\n",
        "                \"f1\": MulticlassF1Score(num_classes=self.out_channels, average=\"macro\"),\n",
        "            }\n",
        "        )\n",
        "        return metrics.clone(prefix=metric_prefix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from abc import ABC\n",
        "from lightning import LightningModule\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "\n",
        "class SSLBase(LightningModule, ABC):\n",
        "    \"\"\"Base model for Self-Supervised Learning (SSL), encapsulates downstream evaluation hooks.\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate: float, weight_decay: float, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.downstream_model = LinearProbingClassifier(out_channels=out_channels)\n",
        "\n",
        "    def on_validation_epoch_start(self) -> None:\n",
        "        \"\"\"Before computing reprs and scores for validation set,\n",
        "        updates downstream model with train reprs.\n",
        "        \"\"\"\n",
        "        self._update_downstream_model_with_train_representations()\n",
        "\n",
        "    def validation_step(self, batch: Tensor, batch_idx: int) -> None:\n",
        "        \"\"\"Computes representations of single validation set batch.\"\"\"\n",
        "        x, y = batch\n",
        "        z = self.forward_repr(x)\n",
        "        self.downstream_model.update_test(z, y)\n",
        "\n",
        "    def on_validation_epoch_end(self) -> None:\n",
        "        \"\"\"Fits downstream model on train set and scores on validation set.\"\"\"\n",
        "        self.downstream_model.fit()\n",
        "        metrics = self.downstream_model.score(metric_prefix=\"val/\")\n",
        "        self.log_dict(metrics)\n",
        "\n",
        "    def on_test_epoch_start(self) -> None:\n",
        "        \"\"\"Before computing reprs and scores for test set,\n",
        "         updates downstream model with train reprs.\n",
        "        \"\"\"\n",
        "        self._update_downstream_model_with_train_representations()\n",
        "\n",
        "    def test_step(self, batch: Tensor, batch_idx: int) -> None:\n",
        "        \"\"\"Computes representations of single test set batch.\"\"\"\n",
        "        x, y = batch\n",
        "        z = self.forward_repr(x)\n",
        "        self.downstream_model.update_test(z, y)\n",
        "\n",
        "    def on_test_epoch_end(self) -> None:\n",
        "        \"\"\"Fits downstream model on train set and scores on test set.\"\"\"\n",
        "        self.downstream_model.fit()\n",
        "        metrics = self.downstream_model.score(metric_prefix=\"test/\")\n",
        "        self.log_dict(metrics)\n",
        "\n",
        "    def _update_downstream_model_with_train_representations(self) -> None:\n",
        "        \"\"\"Resets state of the downstream model and computes representations of the train set.\"\"\"\n",
        "        self.downstream_model.reset()\n",
        "\n",
        "        for batch in self.trainer.datamodule.train_dataloader():  # type: ignore\n",
        "            # Iterating data_loader manually requires manual device change:\n",
        "            x, y = batch\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            z = self.forward_repr(x)\n",
        "            self.downstream_model.update_train(z, y)\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"Sets up the optimizer for the model.\"\"\"\n",
        "        return torch.optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams[\"learning_rate\"],\n",
        "            weight_decay=self.hparams[\"weight_decay\"],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s5NarrwBiJIk"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from json import encoder\n",
        "from torch import le, nn\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "class SmallConvnet(nn.Module):\n",
        "    \"\"\"Small ConvNet (avoids heavy computation).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 21 * 21, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.reshape(-1, 16 * 21 * 21)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim: int, hidden_dim: int, output_dim: int, plain_last: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "        )\n",
        "        if not plain_last:\n",
        "            self.net.append(nn.BatchNorm1d(output_dim))\n",
        "            self.net.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def mlp(dim: int, projection_size: int = 256) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, projection_size),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(projection_size),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "class BYOLModel(SSLBase):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate: float,\n",
        "        weight_decay: float,\n",
        "        tau: float,\n",
        "        out_channels: int = 10,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            out_channels=out_channels,\n",
        "        )\n",
        "\n",
        "        # Initialize online network\n",
        "        # funkcja f\n",
        "        self.online_encoder = SmallConvnet()\n",
        "\n",
        "        # funkcja g\n",
        "        self.online_projector = MLP(84, 84, 84, plain_last=False)\n",
        "\n",
        "        # funkcja q\n",
        "        self.online_predictor = MLP(84, 84, 84, plain_last=True)\n",
        "        self.online_net = nn.Sequential(\n",
        "            self.online_encoder,\n",
        "            self.online_projector,\n",
        "            self.online_predictor,\n",
        "        )\n",
        "\n",
        "        # Initialize target network with frozen weights\n",
        "        self.target_encoder = self.copy_and_freeze_module(self.onl1500ine_encoder)\n",
        "        self.target_projector = self.copy_and_freeze_module(self.online_projector)\n",
        "        self.target_net = nn.Sequential(self.target_encoder, self.target_projector)\n",
        "\n",
        "        # Initialize augmentations\n",
        "        self.aug_1 = get_t1_aug()\n",
        "        self.aug_2 = get_t1_aug()\n",
        "\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
        "        t = self.aug_1(x)\n",
        "        t_prim = self.aug_2(x)\n",
        "\n",
        "        q = self.online_net(t)\n",
        "        q_sym = self.online_net(t_prim)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z_prim = self.target_net(t_prim)\n",
        "            z_prim_sym = self.target_net(t)\n",
        "\n",
        "        q = torch.cat([q, q_sym], dim=0)\n",
        "        z_prim = torch.cat([z_prim, z_prim_sym], dim=0)\n",
        "\n",
        "        return q, z_prim\n",
        "\n",
        "    def forward_repr(self, x: Tensor) -> Tensor:\n",
        "        return self.online_encoder(x)\n",
        "\n",
        "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
        "        x, _ = batch\n",
        "        q, z_prim = self.forward(x)\n",
        "        loss = self.byol_loss(q=q, z_prim=z_prim)\n",
        "\n",
        "        self.log(\"train/loss\", loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def byol_loss(self, q: Tensor, z_prim: Tensor) -> Tensor:\n",
        "        q = F.normalize(q, dim=-1, p=2)\n",
        "        z_prim = F.normalize(z_prim, dim=-1, p=2)\n",
        "        return (2 - 2 * (q * z_prim).sum(dim=-1)).mean()\n",
        "\n",
        "    def on_train_epoch_end(self) -> None:\n",
        "        super().on_train_epoch_end()\n",
        "        self.update_target_network()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_target_network(self) -> None:\n",
        "        for target_param, online_param in zip(\n",
        "            self.target_net.parameters(), self.online_net.parameters()\n",
        "        ):\n",
        "            target_param.data = (\n",
        "                self.tau * target_param.data + (1 - self.tau) * online_param.data\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_and_freeze_module(model: nn.Module) -> nn.Module:\n",
        "        mode_copy = copy.deepcopy(model)\n",
        "        for param in mode_copy.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        return mode_copy\n",
        "\n",
        "\n",
        "aug_1 = get_t1_aug()\n",
        "aug_2 = get_t1_aug()\n",
        "\n",
        "model = BYOLModel(\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-5,\n",
        "    tau=0.99,\n",
        "    out_channels=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name             | Type         | Params\n",
            "--------------------------------------------------\n",
            "0 | online_encoder   | SmallConvnet | 859 K \n",
            "1 | online_projector | MLP          | 14.6 K\n",
            "2 | online_predictor | MLP          | 14.4 K\n",
            "3 | online_net       | Sequential   | 888 K \n",
            "4 | target_encoder   | SmallConvnet | 859 K \n",
            "5 | target_projector | MLP          | 14.6 K\n",
            "6 | target_net       | Sequential   | 874 K \n",
            "--------------------------------------------------\n",
            "888 K     Trainable params\n",
            "874 K     Non-trainable params\n",
            "1.8 M     Total params\n",
            "7.054     Total estimated model params size (MB)\n",
            "/home/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   6%|▋         | 5/79 [00:00<00:03, 20.87it/s, v_num=13, train/loss=0.218]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 199: 100%|██████████| 79/79 [00:03<00:00, 20.61it/s, v_num=13, train/loss=0.200] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 199: 100%|██████████| 79/79 [00:03<00:00, 20.39it/s, v_num=13, train/loss=0.200]\n",
            "{'accuracy': 0.317125, 'precision': 0.30261530578202556, 'recall': 0.317125, 'f1_score': 0.30315973671109153}\n"
          ]
        }
      ],
      "source": [
        "from cgitb import small\n",
        "from lightning import Trainer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from typing import Tuple\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "TAU = 0.99\n",
        "EPOCHS = 200\n",
        "ACCELERATOR = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "OUT_DIR = \"logs\"\n",
        "# logger = TensorBoardLogger(save_dir=OUT_DIR, default_hp_metric=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x = next(iter(train_loader))[0].size()\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    default_root_dir=OUT_DIR,\n",
        "    max_epochs=EPOCHS,\n",
        "    # logger=logger,\n",
        "    accelerator=ACCELERATOR,\n",
        "    num_sanity_val_steps=0,\n",
        "    log_every_n_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(model: BYOLModel, dataloader: DataLoader, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            # x = x.to(device)\n",
        "            feature = model.forward_repr(x)\n",
        "            features.append(feature.cpu())\n",
        "            labels.append(y.cpu())\n",
        "    \n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    \n",
        "    return features, labels\n",
        "\n",
        "# Function to train a classifier and return it\n",
        "def train_classifier(train_features: torch.Tensor, train_labels: torch.Tensor) -> LogisticRegression:\n",
        "    classifier = LogisticRegression(max_iter=1000)\n",
        "    classifier.fit(train_features.numpy(), train_labels.numpy())\n",
        "    return classifier\n",
        "\n",
        "# Function to evaluate a classifier and return metrics\n",
        "def evaluate_classifier(classifier: LogisticRegression, test_features: torch.Tensor, test_labels: torch.Tensor) -> dict:\n",
        "    predictions = classifier.predict(test_features.numpy())\n",
        "    accuracy = accuracy_score(test_labels.numpy(), predictions)\n",
        "    precision = precision_score(test_labels.numpy(), predictions, average='weighted')\n",
        "    recall = recall_score(test_labels.numpy(), predictions, average='weighted')\n",
        "    f1 = f1_score(test_labels.numpy(), predictions, average='weighted')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Main function to get classification metrics\n",
        "def get_classification_metrics(model: BYOLModel, train_loader: DataLoader, test_loader: DataLoader, device: torch.device) -> dict:\n",
        "    # Extract features\n",
        "    train_features, train_labels = extract_features(model, train_loader, device)\n",
        "    test_features, test_labels = extract_features(model, test_loader, device)\n",
        "    \n",
        "    # Train classifier\n",
        "    classifier = train_classifier(train_features, train_labels)\n",
        "    \n",
        "    # Evaluate classifier\n",
        "    metrics = evaluate_classifier(classifier, test_features, test_labels)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "trainer.fit(model, train_loader)\n",
        "# test_model(model, test_loader)\n",
        "metrics = get_classification_metrics(model, train_loader, test_loader, device)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qitno0wc8W35"
      },
      "source": [
        "## Douczanie nadzorowane\n",
        "\n",
        "***Zaimplementuj*** proces douczania kodera z poprzedniego polecenia na danych etykietowanych ze zbioru treningowego. Porównaj jakość tego modelu z modelem nauczonym tylko na danych etykietownaych. Postaraj się wyjaśnić różnice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BiMQc6aZXgI_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 2.0489901379693913\n",
            "Epoch 2/50, Loss: 1.7120867469642735\n",
            "Epoch 3/50, Loss: 1.5818697063228753\n",
            "Epoch 4/50, Loss: 1.4690142703961722\n",
            "Epoch 5/50, Loss: 1.3723201480092881\n",
            "Epoch 6/50, Loss: 1.2746228273910811\n",
            "Epoch 7/50, Loss: 1.1930405105216593\n",
            "Epoch 8/50, Loss: 1.0919697096076193\n",
            "Epoch 9/50, Loss: 1.0074932892111284\n",
            "Epoch 10/50, Loss: 0.9136613056629519\n",
            "Epoch 11/50, Loss: 0.8540321573426451\n",
            "Epoch 12/50, Loss: 0.7474737189993074\n",
            "Epoch 13/50, Loss: 0.6759019780762588\n",
            "Epoch 14/50, Loss: 0.5989937050433098\n",
            "Epoch 15/50, Loss: 0.5400493642951869\n",
            "Epoch 16/50, Loss: 0.45639691134042376\n",
            "Epoch 17/50, Loss: 0.39345170142529884\n",
            "Epoch 18/50, Loss: 0.3364166047754167\n",
            "Epoch 19/50, Loss: 0.2726752407188657\n",
            "Epoch 20/50, Loss: 0.23981903446248815\n",
            "Epoch 21/50, Loss: 0.20362177745828144\n",
            "Epoch 22/50, Loss: 0.1698009419856192\n",
            "Epoch 23/50, Loss: 0.1350045704954787\n",
            "Epoch 24/50, Loss: 0.1070497921561893\n",
            "Epoch 25/50, Loss: 0.09287465050156359\n",
            "Epoch 26/50, Loss: 0.09468121010857293\n",
            "Epoch 27/50, Loss: 0.07400144504595406\n",
            "Epoch 28/50, Loss: 0.06034990397718134\n",
            "Epoch 29/50, Loss: 0.09609390960275373\n",
            "Epoch 30/50, Loss: 0.16423226980185962\n",
            "Epoch 31/50, Loss: 0.0756513196503437\n",
            "Epoch 32/50, Loss: 0.04406715433195799\n",
            "Epoch 33/50, Loss: 0.02256429964039899\n",
            "Epoch 34/50, Loss: 0.015845620851445048\n",
            "Epoch 35/50, Loss: 0.009917455993569161\n",
            "Epoch 36/50, Loss: 0.00773836128543355\n",
            "Epoch 37/50, Loss: 0.006286448040163687\n",
            "Epoch 38/50, Loss: 0.00516564751786616\n",
            "Epoch 39/50, Loss: 0.004292064547521074\n",
            "Epoch 40/50, Loss: 0.0037109386183238956\n",
            "Epoch 41/50, Loss: 0.0032928107207629218\n",
            "Epoch 42/50, Loss: 0.0028615283678120732\n",
            "Epoch 43/50, Loss: 0.0025444057286728786\n",
            "Epoch 44/50, Loss: 0.0022882384136083383\n",
            "Epoch 45/50, Loss: 0.0020057256046175673\n",
            "Epoch 46/50, Loss: 0.0017999105742702237\n",
            "Epoch 47/50, Loss: 0.0016189777539877951\n",
            "Epoch 48/50, Loss: 0.0014978605359380098\n",
            "Epoch 49/50, Loss: 0.0013647038284011327\n",
            "Epoch 50/50, Loss: 0.0012125111519524097\n",
            "Accuracy: 0.429375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.65      0.68      0.67       800\n",
            "        bird       0.33      0.30      0.31       800\n",
            "         car       0.62      0.61      0.61       800\n",
            "         cat       0.27      0.29      0.28       800\n",
            "        deer       0.35      0.35      0.35       800\n",
            "         dog       0.23      0.24      0.23       800\n",
            "       horse       0.40      0.37      0.38       800\n",
            "      monkey       0.34      0.32      0.33       800\n",
            "        ship       0.60      0.64      0.62       800\n",
            "       truck       0.50      0.49      0.49       800\n",
            "\n",
            "    accuracy                           0.43      8000\n",
            "   macro avg       0.43      0.43      0.43      8000\n",
            "weighted avg       0.43      0.43      0.43      8000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "state_dict = model.online_encoder.state_dict()\n",
        "encoder = SmallConvnet()\n",
        "encoder.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "class SupervisedModel(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.fc = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "supervised_model = SupervisedModel(encoder)\n",
        "supervised_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(supervised_model.parameters(), lr=0.001)\n",
        "\n",
        "train(supervised_model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "evaluate(supervised_model, test_loader)\n",
        "evaluate_classification_report(supervised_model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
