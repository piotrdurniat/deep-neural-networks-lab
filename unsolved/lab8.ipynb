{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60IzlMsuY2w4"
      },
      "source": [
        "# Wstęp\n",
        "Zadanie 8 obejmuje zagadnienie segmentacji obrazów i wprowadza konwolucyjne modele o architekturze enkoder-dekoder. Modele takie wykorzystują konwolucje i downsampling aby uzyskać mapę cech o mniejszej rozdzielczości (enkoder), a następnie \"dekodują\" otrzymaną ją z powrotem do większej rozdzielczości aby wydobyć w rozdzielczości oryginalnego obrazka interesujące nas informacje semantyczne - np. segmentację obrazka na poszczególne obiekty. Architektury tego typu mają również zastosowania w modelach generatywnych, w zadaniach które ogólnie określamy jako *image to image translation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWre4lMJesof"
      },
      "source": [
        "# Cel ćwiczenia\n",
        "\n",
        "Celem ćwiczenia jest poznanie\n",
        "\n",
        "\n",
        "\n",
        "*   metod upsamplingu w sieciach konwolucyjnych\n",
        "*   architektur konwolucyjnych o strukturze enkoder-dekoder\n",
        "*   wykorzystania sieci głębokich w segmentacji danych obrazowych\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR_sgwz9lWmi"
      },
      "source": [
        "# Upsampling w sieciach konwolucyjnych\n",
        "\n",
        "Na laboratorium 5 dowiedzieliśmy się już, że do zmniejszania rozdzielczości mapy cech możemy wykorzystać dwa podejścia - pooling lub warstwę konwolucyjną z odpowiednim parametrem `stride`. Podobnie jest ze zwiększaniem rozdzielczości.\n",
        "\n",
        "Wersja prostsza, odwrotność poolingu, to `torch.nn.Upsample`. Jest to pozbawiona uczących się parametrów klasa dająca wybór trybu interpolacji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDwfWomRl8wL",
        "outputId": "8161135a-6b98-427b-a353-735dc1fe63a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 64, 64])"
            ]
          },
          "execution_count": 1,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "upsampler = torch.nn.Upsample(scale_factor=2)\n",
        "\n",
        "dummy_input = torch.zeros((3, 3, 32, 32))\n",
        "upsampler(dummy_input).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utxlxvxcl8ar"
      },
      "source": [
        "Klasa `torch.nn.ConvTranspose2d` to natomiast ucząca się operacja \"odwracania\" konwolucji. Jej relacja wymiarów wejście-wyjście powinna być taka, jak wyjście do wejścia dla warstwy `torch.nn.Conv2d` o analogicznych parametrach konstruktora. Innymi słowy, jeśli tensor przepuścimy przez konwolucję, a następnie konwolucję transponowaną, oczekujemy powrotu do początkowej szerokości i wysokości - o ile oczywiście parametry tych dwóch warstw będą się ze sobą zgadzać.\n",
        "\n",
        "Konwolucja transponowana wprowadza dodatkowy parametr `output_padding`. Jest on potrzebny dlatego, że ze względu na zaokrąglanie wymiarów dla zwyczajnej konwolucji wykorzystującej `stride>1`, na podstawie wymiaru jej wyjścia nie da się jednoznacznie określić wymiaru jej wejścia. Aby spełnić założenie że warstwa `torch.nn.ConvTranspose2d` odwraca zmianę wymiarów której dokonuje `torch.nn.Conv2d`, potrzebny jest więc dodatkowy parametr pozwalający doprecyzować oczekiwany rozmiar. W razie problemów z uzyskaniem właściwego rozmiaru, dokumentacja torcha podaje konkrenty wzór na zależność między parametrami warstwy a szeokością i wysokością jej wyniku.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_3YLljknltt"
      },
      "outputs": [],
      "source": [
        "upsampler = torch.nn.ConvTranspose2d(3, 3, 2, stride=2)\n",
        "\n",
        "dummy_input = torch.zeros((3, 3, 32, 32))\n",
        "\n",
        "print(f\"kształt po upsamplingu: {upsampler(dummy_input).shape}\")\n",
        "\n",
        "conv = torch.nn.Conv2d(3, 3, 3, stride=2, padding=1)\n",
        "deconv = torch.nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1)\n",
        "fixed_deconv = torch.nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "print(f\"kształt po konwolucji: {conv(dummy_input).shape}\")\n",
        "print(\n",
        "    f\"kształt po konwolucji i konw. transponowanej: {deconv(conv(dummy_input)).shape}\"\n",
        ")\n",
        "print(f\"j.w., ale z output_padding: {fixed_deconv(conv(dummy_input)).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1A6SuJZqgBg"
      },
      "source": [
        "# Segmentacja obrazu\n",
        "\n",
        "Model enkoder-dekoder testować będziemy w zadaniu nadzorowanej segmentacji. Oznacza to, że dla obrazu wejściowego musimy znaleźć etykiety ze z góry wyznaczonego zbioru, przewidujemy je jednak nie dla całego obrazu, a na poziomie każdego piksela. Skorzystamy z dostępnego w `torchvision.datasets` zbioru VOC. Najpierw jednak musimy zdefiniować transformację danych, kóra będzie nieco bardziej skomplikowana niż do tej pory.\n",
        "\n",
        "Ponieważ etykiety są podane również jako obraz, trzeba pamiętać, że augmentacje losowe muszą zgadzać się pomiędzy obrazkiem a etykietą. Niestety, każde wywołanie funkcji w standardowym przepływie danych z wykorzystaniem `transforms` losuje augmentację na nowo. Aby uzyskać identyczną augmentację, konieczne będzie wykorzystanie funkcji z `torchvision.transforms.functional`.\n",
        "\n",
        "Druga istotna zmiana to konwersja do tensora. Aby zachować `target` jako tensor całkowitoliczbowy, musimy zastosować własną alternatywę dla ToTensor(), jako że `ToTensor` zawsze przekształca obrazy do tensora zmiennmoprzecinkowego w zakresie [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZiK62OuxsoB"
      },
      "source": [
        "# Zadanie 1\n",
        "\n",
        "Zdefiniuj transformację, która dla pary wejściowej `image, target` zwraca zaugmentowane w identyczny sposób obraz i etykiety. Transformacja powinna obejmować:\n",
        "\n",
        "*   dowolną augmentację na danych wejściowych\n",
        "*   padding to stałego rozmiaru (w przypadku etykiet, padować trzeba wartością 255)\n",
        "*   konwersję do tensora - zmiennoprzecinkowego dla obrazu, całkowitoliczbowego dla etykiety\n",
        "\n",
        "Zdefiniuj również odpowiednią transformację dla zbioru testowego, pamiętając że dane testowe nie powinny byc augmentowane.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd2R0l8aajz6"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "\n",
        "def pair_transforms(image, target):\n",
        "\n",
        "    # początkowy resize jest dokonany żeby zmniejszyć koszt obliczeniowy na potrzeby ograniczeń laboratorium\n",
        "    # standardowo pracując na zbiorze nie dokonalibyśmy tego przekształcenia!\n",
        "    resize = transforms.Resize(\n",
        "        (image.size[1] // 4, image.size[0] // 4), PIL.Image.NEAREST\n",
        "    )\n",
        "    image = resize(image)\n",
        "    target = resize(target)\n",
        "\n",
        "    #\n",
        "\n",
        "    # ponieważ VOC jest trudnym zbiorem, przedefiniujmy problem\n",
        "    # zamiast identyfikować wszystkie \"rozpoznawalne\" obiekty, tworzymy etykiety klasyfikacji tło-obiekt\n",
        "    out_target = torch.where(\n",
        "        torch.logical_or(out_target == 0, out_target == 255), out_target, 1\n",
        "    )\n",
        "\n",
        "    return out_image, out_target\n",
        "\n",
        "\n",
        "def test_pair_transforms(image, target):\n",
        "\n",
        "    resize = transforms.Resize(\n",
        "        (image.size[1] // 4, image.size[0] // 4), PIL.Image.NEAREST\n",
        "    )\n",
        "    image = resize(image)\n",
        "    target = resize(target)\n",
        "\n",
        "    #\n",
        "\n",
        "    out_target = torch.where(\n",
        "        torch.logical_or(out_target == 0, out_target == 255), out_target, 1\n",
        "    )\n",
        "\n",
        "    return out_image, out_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61YHGov9q1mv"
      },
      "source": [
        "Załaduj zbiór danych ze zeefiniowaną transformacją, a następnie zweryfikuj, że augmentacja zgadza się pomiędzy obrazem a etykietą. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "cd93df2e723b4782a45dede41b61fc43",
            "1585a8cb5bff49e695c13d010cf637c9",
            "f7275003dd4c40368357a9307a1c3dce",
            "1a2b03f75b5d4fe480e3c6eccf76d8fe",
            "ab7e0f608e074ad089a182ae58aff63f",
            "d0cfde9f29ef44cb9922a9c5edb1fe2e",
            "2bab9efc11ef445984bca4ddc014aae6",
            "9dc8519fe220485783329e37f1935f45"
          ]
        },
        "id": "JwTjN_rvh1HY",
        "outputId": "a2836bcd-92b0-4414-a6f0-3291a5f40f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to root/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd93df2e723b4782a45dede41b61fc43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1999639040.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting root/VOCtrainval_11-May-2012.tar to root\n"
          ]
        }
      ],
      "source": [
        "data = datasets.VOCSegmentation(\"root\", download=True, transforms=pair_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAaJ5hl9fFWi"
      },
      "outputs": [],
      "source": [
        "test_data = datasets.VOCSegmentation(\n",
        "    \"root\", download=True, image_set=\"val\", transforms=test_pair_transforms\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjsiQptQcMCS"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "image, label = data[0]\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.show()\n",
        "plt.imshow(label)\n",
        "plt.show()\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlFMUusBsaxN"
      },
      "source": [
        "Zauważ, że wartością 255 oznaczone są \"krawędzie\" pomiędzy właściwymi etykietami. Etykieta 255 nie powinna być brana pod uwagę przy wyliczaniu kosztu i metryk jakości segmentacji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd_Crryr3BAs"
      },
      "source": [
        "# Architektura enkoder-dekoder\n",
        "\n",
        "\n",
        "Możemy przejść do zdefiniowania architektury enkoder-dekoder. Achitektura ta powinna przetworzyć obraz do niższej rozdzielczości, a następnie zwiększyć jego wymiar z powrotem do oryginalnego. Podobnie jak w standardowych sieciach konwolucyjnych do klasyfikacji, przydatne jest zwiększanie liczby kanałów wraz ze zmniejszaniem rozdzielczości. Pozwala to na zachowanie informacji po downsamplingu, oraz daje możliwość wyuczenia najbardziej złożonych zależności tam gdzie jest to najbardziej istotne, czyli w warstwach operujących na najniższej rozdzielczości gdzie pole recepcyjne filtra obejmuje największy fragment obrazu. Przy upsamplingu, liczba kanałów typowo będzie się zmniejszać. Dodatkowo, skrótowe połączenia według schematu: pierwsza warstwa do ostatniej, druga do przedostaniej etc. są typowym elementem takich architektur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_QcHzmmqSwG"
      },
      "source": [
        "# Zadanie 2\n",
        "\n",
        "Zmodyfikuj funkcję forward klasy SimpleEncoderDecoder tak, aby dodać połączenia skrótowe między warstwami o tej samej rozdzielczości.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmNTBPI-wJLN"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class SimpleEncoderDecoder(torch.nn.Module):\n",
        "    def __init__(self, channels, out_labels):\n",
        "        super().__init__()\n",
        "        self.conv_1 = torch.nn.Conv2d(3, channels[0], 3, padding=1)\n",
        "        self.conv_2 = torch.nn.Conv2d(channels[0], channels[1], 3, padding=1)\n",
        "        self.conv_3 = torch.nn.Conv2d(channels[1], channels[2], 3, padding=1)\n",
        "        self.conv_4 = torch.nn.Conv2d(channels[2], channels[2], 3, padding=1)\n",
        "        self.up_conv1 = torch.nn.ConvTranspose2d(channels[2], channels[1], 2, stride=2)\n",
        "        self.up_conv2 = torch.nn.ConvTranspose2d(channels[1], channels[0], 2, stride=2)\n",
        "        self.up_conv3 = torch.nn.ConvTranspose2d(channels[0], out_labels, 2, stride=2)\n",
        "        self.pool = torch.nn.MaxPool2d(2)\n",
        "        self.act = torch.nn.ReLU()\n",
        "        self.bnorm = torch.nn.BatchNorm2d(channels[2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.conv_1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.act(self.conv_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.act(self.conv_3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bnorm(x)\n",
        "        x = self.act(self.conv_4(x))\n",
        "        x = self.act(self.up_conv1(x))\n",
        "        x = self.act(self.up_conv2(x))\n",
        "        x = self.act(self.up_conv3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hKbugh8_iH6"
      },
      "source": [
        "Zweryfikuj działanie sieci:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgs1IXSA_mLy"
      },
      "outputs": [],
      "source": [
        "model = SimpleEncoderDecoder([32, 64, 128], 21).cuda()\n",
        "\n",
        "dummy_data = torch.zeros((32, 3, 64, 64)).cuda()\n",
        "\n",
        "assert model(dummy_data).shape == (32, 21, 64, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN00sFAb74QO"
      },
      "source": [
        "Zdefinujmy kod pomocniczy - jak zwykle można zastąpić własnym, uwzględniającym ulepszenia z poprzednich laboratoriów. Zauważ, że ze funkcja entropii krzyżowej nie wymaga adaptacji do zadania. Natomiast implementacja `count_correct` została zmieniona tak, aby nie zliczać pikseli oznaczonych wartością 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1W7wDlh5se4"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def count_correct(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "    preds = torch.argmax(y_pred, dim=1)\n",
        "    not_bg = (y_true != 255).float()\n",
        "    return ((preds == y_true).float() * not_bg).sum() / not_bg.sum()\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model: nn.Module, loss_fn: torch.nn.CrossEntropyLoss, dataloader: DataLoader\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    all = 0\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        y_pred = model(X_batch.cuda())\n",
        "        all += 1\n",
        "        loss += loss_fn(y_pred, y_batch.cuda()).sum()\n",
        "        correct += count_correct(y_pred, y_batch.cuda())\n",
        "    return loss / all, correct / all\n",
        "\n",
        "\n",
        "def fit(\n",
        "    model: nn.Module,\n",
        "    optimiser: optim.Optimizer,\n",
        "    loss_fn: torch.nn.CrossEntropyLoss,\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader,\n",
        "    epochs: int,\n",
        "    print_metrics: str = True,\n",
        "):\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_dl:\n",
        "            y_pred = model(X_batch.cuda())\n",
        "            loss = loss_fn(y_pred, y_batch.cuda())\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "        if print_metrics:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_loss, train_acc = validate(\n",
        "                    model=model, loss_fn=loss_fn, dataloader=train_dl\n",
        "                )\n",
        "                val_loss, val_acc = validate(\n",
        "                    model=model, loss_fn=loss_fn, dataloader=val_dl\n",
        "                )\n",
        "                print(\n",
        "                    f\"Epoch {epoch}: \"\n",
        "                    f\"train loss = {train_loss:.3f} (acc: {train_acc:.3f}), \"\n",
        "                    f\"validation loss = {val_loss:.3f} (acc: {val_acc:.3f})\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPh6_3GN8qbz"
      },
      "source": [
        "Wyucz sieć na danych z VOCSegmentation.\n",
        "\n",
        "Uwaga: VOCSegmentation to dość trudny zbiór, nie trzeba tu osiągać wyjątkowo dobrych wyników.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdpVDFUt8uc6"
      },
      "outputs": [],
      "source": [
        "model = SimpleEncoderDecoder([32, 64, 128], 2).cuda()\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "train_dl = DataLoader(data, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test_data, batch_size=32)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, optimizer, loss, train_dl, test_dl, 50, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr8S8FTfvWuD"
      },
      "source": [
        "# Zadanie 3\n",
        "\n",
        "Zdefiniuj parametryzowalną klasę EncoderDecoder, umożliwiającą wybór liczby warstw i liczby kanałów w poszczególnych warstwach. Klasa powinna dodawać połączenia skrótowe pomiędzy warstwami w początkowej a końcowej części architektury o tej samej rozdzielczości. Połączenia skrótowe powinny działać przez konkatenację.\n",
        "\n",
        "Klasa powinna umożlwiać wybór pomiędzy upsamplingiem z wykorzsytaniem `ConvTranspose2D` a `torch.nn.Upsample`. Sprawdź wyniki tej drugiej opcji z parametrami analogicznymi do testów `SimpleEncoderDecoder` na segmentacji VOC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giFyVcpXZvN3"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(torch.nn.Module):\n",
        "  def __init__(self, channels, out_labels):\n",
        "    #\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab8.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1585a8cb5bff49e695c13d010cf637c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2b03f75b5d4fe480e3c6eccf76d8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc8519fe220485783329e37f1935f45",
            "placeholder": "​",
            "style": "IPY_MODEL_2bab9efc11ef445984bca4ddc014aae6",
            "value": " 1999639552/? [02:09&lt;00:00, 15450367.53it/s]"
          }
        },
        "2bab9efc11ef445984bca4ddc014aae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc8519fe220485783329e37f1935f45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7e0f608e074ad089a182ae58aff63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "cd93df2e723b4782a45dede41b61fc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7275003dd4c40368357a9307a1c3dce",
              "IPY_MODEL_1a2b03f75b5d4fe480e3c6eccf76d8fe"
            ],
            "layout": "IPY_MODEL_1585a8cb5bff49e695c13d010cf637c9"
          }
        },
        "d0cfde9f29ef44cb9922a9c5edb1fe2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7275003dd4c40368357a9307a1c3dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0cfde9f29ef44cb9922a9c5edb1fe2e",
            "max": 1999639040,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab7e0f608e074ad089a182ae58aff63f",
            "value": 1999639040
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
